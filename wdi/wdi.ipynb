{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Paquetes de instalacion\n",
    "'''!conda install -c anaconda pandas\n",
    "!conda install -c anaconda cloudpickle\n",
    "!conda install -c anaconda numpy\n",
    "!conda install -c conda-forge matplotlib\n",
    "!conda install -c anaconda seaborn\n",
    "!conda install -c anaconda scipy\n",
    "!conda install -c conda-forge python-pdfkit\n",
    "!conda install -c anaconda statsmodels\n",
    "!conda install -c anaconda xlrd'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os,inspect, gc\n",
    "import sys\n",
    "import re\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_file = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "path_file=os.path.join(path_file,'datos_reales')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Modulos de lectura y guardado de archivos csv y xlsx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excelFile(file)\n",
    "    #retorna xlsx file, util para archivos excel con multiples pestañas\n",
    "#parameters:\n",
    "    #file: string\n",
    "           #nombre del archivo excel\n",
    "def excelFile(file):\n",
    "    return pd.ExcelFile(open(os.path.join(path_file,file), 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#readFile(file, sheet=None)\n",
    "    #retorna data frame\n",
    "#parameters:\n",
    "    #file: string\n",
    "        #nombre del archivo excel o csv\n",
    "    #sheet: String\n",
    "        #nombre de una pestaña en la hoja excel\n",
    "def readFile(file, sheet=None):\n",
    "    if file.endswith('.xlsx') or file.endswith('.xls'):\n",
    "        if sheet == None:\n",
    "            raise Exception('sheet name should be defined')\n",
    "        else:\n",
    "            return pd.read_excel(open(os.path.join(path_file,file), 'rb'), sheet_name=sheet)\n",
    "    elif file.endswith('.csv'):\n",
    "        return pd.read_csv(os.path.join(path_file,file))\n",
    "    else:\n",
    "        raise Exception('Types would be 0 or 1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saveFile(dataFrame, file, sheet='hoja1')\n",
    "    #Guarda un data frame en un archivo excel o csv\n",
    "#parameters:\n",
    "    #dataFrame: dataFrame\n",
    "        #data frame a guardar\n",
    "    #out: string\n",
    "        #nombre del resultado xlsx o csv\n",
    "    #sheet: String\n",
    "        #nombre de una pestaña en la hoja excel\n",
    "def saveFile(dataFrame, out, sheet='hoja1'):\n",
    "    if out.endswith('.xlsx') or out.endswith('.xls'):\n",
    "        if sheet == None:\n",
    "            raise Exception('sheet name should be gived')\n",
    "        else:\n",
    "            dataFrame.to_excel(\"datos_reales/\"+out, sheet_name=sheet)\n",
    "    elif out.endswith('.csv'):\n",
    "        dataFrame.to_csv(\"datos_reales/\"+out)\n",
    "    else:\n",
    "        raise Exception('Support only .xlsx, .xls, .csv files')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocesamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitByColumn(dataFrame, by, out)\n",
    "        #separa un dataFrame mediante grupos y retorna o guarda cada grupo en un excel segun frame\n",
    "#Parameters:\n",
    "    #dataFrame: dataFrame\n",
    "        #dataframe a separar y guardar\n",
    "    #by: String\n",
    "        #columna mediante el cual generar grupos\n",
    "    #out: String\n",
    "        #nombre del resultado xlsx, por defecto nombre de la funcion\n",
    "    #frame: Boolean\n",
    "        #Si es true retorna el dataframe agrupado mediante by, caso contrario, guarda el dataframe agrupado por grupos en un solo excel\n",
    "def splitByColumn(dataFrame, by, out=\"splitByColumn.xlsx\", frame=False):\n",
    "    if frame:\n",
    "        return dataFrame.groupby(by=by)\n",
    "    else:\n",
    "        with pd.ExcelWriter('datos_reales/'+out) as writer:\n",
    "            for key, group in dataFrame.groupby(by=by):\n",
    "                group.reset_index(drop=True).to_excel(writer, sheet_name=key, index=True)\n",
    "            \n",
    "#data=readFile('hoja1_WDIEXCEL.csv')\n",
    "#splitByColumn(data, \"Country Code\", frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def countMissingDataPerYear(dataFrame, initYear, endYear)\n",
    "    # retorna un dataFrame, cuenta el numero de datos faltantes (nan y cero) por año desde un año inicial hasta uno final (años en las columnas)\n",
    "#Parameters:\n",
    "    #datFrame: Dataframe\n",
    "        #dataframe a analizar\n",
    "    #initYear: Int\n",
    "        #Año inicial\n",
    "    #endYear: Int\n",
    "        #Año final\n",
    "def countMissingDataPerYear(dataFrame, initYear, endYear):\n",
    "    dataFrame = dataFrame.replace(0, np.nan)\n",
    "    columns = list(dataFrame.columns)\n",
    "    init_year = columns.index(str(initYear))\n",
    "    end_year = columns.index(str(endYear))\n",
    "    total = dataFrame.shape[0]\n",
    "    data_frame_year = dataFrame.iloc[:,init_year:end_year+1]\n",
    "    count_nan = data_frame_year.isna().sum(axis=0)\n",
    "    percent_nan = (100 * count_nan)/total\n",
    "    data_frame = pd.DataFrame(count_nan, columns=['# Nan values'])\n",
    "    data_frame[\"100%\"] = total\n",
    "    data_frame[\"Missing%\"] = percent_nan\n",
    "    return data_frame\n",
    "#data=readFile('splitByColumn.xlsx', \"BRA\")\n",
    "#data = countMissingDataPerYear(data,1960, 2018)\n",
    "#saveFile(data, \"countMissingDataPerYear.xlsx\", sheet='BRA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def countMissingDataPerIndicator(dataFrame, initYear, endYear)\n",
    "    # retorna un dataFrame, cuenta el numero de datos faltantes(ceros y nan) por indicador desde un año inicial hasta uno final (años en las columnas)\n",
    "#Parameters:\n",
    "    #datFrame: Dataframe\n",
    "        #dataframe a analizar\n",
    "    #initYear: Int\n",
    "        #Año inicial\n",
    "    #endYear: Int\n",
    "        #Año final\n",
    "def countMissingDataPerIndicator(dataFrame, initYear, endYear):\n",
    "    dataFrame = dataFrame.replace(0, np.nan)\n",
    "    columns = list(dataFrame.columns)\n",
    "    init_year = columns.index(str(initYear))\n",
    "    end_year = columns.index(str(endYear))\n",
    "    data_frame_year = dataFrame.iloc[:,init_year:end_year+1]\n",
    "    total = data_frame_year.shape[1]\n",
    "    count_nan = data_frame_year.isna().sum(axis=1)\n",
    "    percent_nan = (100 * count_nan)/total\n",
    "    data_frame = pd.DataFrame(count_nan, columns=['# Nan values'])\n",
    "    data_frame[\"100%\"] = total\n",
    "    data_frame[\"Missing%\"] = percent_nan\n",
    "    data_frame.index = dataFrame[\"Indicator Code\"]\n",
    "    return data_frame.sort_values(by=\"Missing%\", ascending=False)\n",
    "#data=readFile('splitByColumn.xlsx', \"BRA\")\n",
    "#data = countMissingDataPerIndicator(data,1990, 2017)\n",
    "#saveFile(data, \"countMissingDataPerIndicator.xlsx\", sheet='hoja1')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame \n",
    "Sera utilizado para todo el analisis, contiene unicamente años e indicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataFrame(dataFrame, initYear, endYear, transpose=False)\n",
    "    #Retorna dataFrame entre años especificados, unicamente contiene años e indicadores y se remplaza todo los valores 0 con NAN\n",
    "#Parameters:\n",
    "    #dataFrame: DataFrame\n",
    "    #initYear: Int\n",
    "        #año desde comenzara el data frame\n",
    "    #endYear: Int\n",
    "        #año donde concluira el data frame\n",
    "    #transpose: Bool\n",
    "        #Si es true retorna un dataFrame[index(year), columns(indicators)] caso contrario dataFrame[index(indicators), columns(year)]\n",
    "def dataFrame(dataFrame, initYear, endYear, transpose=True):\n",
    "    dataFrame = dataFrame.replace(0, np.nan)\n",
    "    columns = list(dataFrame.columns)\n",
    "    init_year = columns.index(str(initYear))\n",
    "    end_year = columns.index(str(endYear))\n",
    "    data_frame_year = dataFrame.iloc[:,init_year:end_year+1]\n",
    "    data_frame_year.index = dataFrame[\"Indicator Code\"]\n",
    "    if transpose:\n",
    "        data_frame_year = data_frame_year.T\n",
    "        #newdata = newdata.reset_index()\n",
    "    return data_frame_year\n",
    "\n",
    "#data = readFile('splitByColumn.xlsx',\"BRA\")\n",
    "#data= dataFrame(data, 1990,2017)\n",
    "#data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ind. <= x% \n",
    "porcentaje permitido de datos faltantes $Ind<p\\%$ en un indicador<br>\n",
    "$\\huge umbral=\\frac{(numAños * P\\%)}{100}$ <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delIndicatorMayorUmbralZerosOrNan(dataFrame, p=0)\n",
    "    #retorna dataFrame eliminando aquellas columnas que contienen 0 or Nan mayor o igual que el umbral \n",
    "#Parameters:\n",
    "    #dataFrame: DataFrame\n",
    "        #dataframe con indicadores en las columnas y años en los indices\n",
    "    #p: int entre 0 y 100\n",
    "        # define el numero de ceros o nan permitidos\n",
    "def delIndicatorMayorUmbralZerosOrNan(dataFrame, p=0):\n",
    "    years = dataFrame.shape[0]\n",
    "    umbral = round((years*p)/100)\n",
    "    #data_frame = (dataFrame==0).astype(int)\n",
    "    data_frame = dataFrame.loc[:,dataFrame.isna().sum(axis=0)<umbral]\n",
    "    data_frame.index.name=\"year\"\n",
    "    return data_frame\n",
    "\n",
    "\n",
    "#data = readFile('splitByColumn.xlsx',\"BRA\")\n",
    "#data= dataFrame(data, 1990,2017)\n",
    "#data=delIndicatorMayorUmbralZerosOrNan(data,25)\n",
    "#saveFile(data, \"delIndicatorMayorUmbralZerosOrNan.xlsx\", sheet='BRA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#contrast = lambda x: True if (x.replace(0,np.nan).dropna().values == x.replace(0,np.nan).dropna().values[0]).all() else False\n",
    "    #Remplaza los ceros por NAN y luego los elimina, luego compara toda la lista de valores con su valor en la posicion 0\n",
    "    #Retorna los indicadores sin cambios en el tiempo\n",
    "#def delIndicatorWithoutChanges(dataFrame, opcion = True)\n",
    "    #retorna dataFrame\n",
    "#Parameters:\n",
    "    #dataFrame: DataFrame\n",
    "    #opcion: Bool\n",
    "        #Si es True retorna los indicadores constantes, caso contrario retorna el dataFrame sin incluir los valores constantes\n",
    "contrast = lambda x: True if (x.replace(0,np.nan).dropna().values == x.replace(0,np.nan).dropna().values[0]).all() else False\n",
    "def delIndicatorWithoutChanges(dataFrame, opcion = True):\n",
    "    data_frame = dataFrame.set_index(\"year\")\n",
    "    booleans = data_frame.apply(contrast)\n",
    "    if opcion:\n",
    "        trues  = booleans[booleans]\n",
    "        return data_frame.loc[:,trues.index]\n",
    "    else:\n",
    "        booleans = np.invert(booleans)\n",
    "        trues  = booleans[booleans]\n",
    "        return data_frame.loc[:,trues.index]\n",
    "\n",
    "\n",
    "#data = readFile('delIndicatorMayorUmbralZerosOrNan.xlsx',\"BRA\")\n",
    "#data = delIndicatorWithoutChanges(data,False)\n",
    "#saveFile(data, \"delIndicatorWithoutChanges.xlsx\", sheet='BRA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#outlier(dataFrame)\n",
    "    #Retorna dos DataFrame(sinAtipicos, soloAtipicos), ademas elimina aquellas columnas que al eliminar el outlier resulta ser todo vacio\n",
    "#Parameters:\n",
    "    #dataFrame:DataFrame\n",
    "def outlier(dataFrame):\n",
    "    data_frame = dataFrame.replace(0,np.nan)\n",
    "    data_frame_outlier = data_frame.copy()\n",
    "    quantiles = data_frame.quantile([0.25, 0.5, 0.75], axis=0)\n",
    "    iqr = quantiles.iloc[2,:] - quantiles.iloc[0,:]\n",
    "    minimun = quantiles.iloc[0,:] - 1.5*iqr\n",
    "    maximun =  quantiles.iloc[2,:] + 1.5*iqr\n",
    "    #dataframe sin valores atipicos\n",
    "    data_frame = data_frame[data_frame>minimun]\n",
    "    data_frame = data_frame[data_frame<maximun]\n",
    "    data_frame = data_frame.dropna(axis=1, how=\"all\")\n",
    "    trues = [col for col in data_frame_outlier.columns if col not in data_frame.columns]\n",
    "    #dataframe con valores atipicos\n",
    "    data_frame_outlier = data_frame_outlier.loc[:,trues]\n",
    "    #data_frame_outlier = data_frame_outlier[data_frame_outlier<minimun]\n",
    "    \n",
    "    return data_frame, data_frame_outlier\n",
    "\n",
    "#data=readFile(\"delIndicatorWithoutChanges.xlsx\", \"BRA\").set_index(\"year\")\n",
    "#data, data_atipicos = outlier(data)\n",
    "#saveFile(data, \"outlier.xlsx\", sheet='BRA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correlation coefficient\n",
    "Este coeficiente sólo debe utilizarse para comparar variables cuantitativas y continuas<br>\n",
    "$\\huge r=\\frac{\\sum_{i=1}^{n}(x_{i}-\\bar{x})(y_{i}-\\bar{y})}{\\sqrt{\\sum_{i=1}^{n}(x_{i}-\\bar{x})^{2}\\sum_{i=1}^{n}(y_{i}-\\bar{y})^{2}}}$ <br>\n",
    "el rango varia entre [-1,1] donde: 1 indica un alto grado de correlacion directa, -1 un alto grado de correlacion inversa y 0 no\n",
    "existe correlacion lineal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joinSeriesAndWdi(series, wdi)\n",
    "    #retorna dataFrame, se realiza union entre dos dataFrames\n",
    "#Parameters:\n",
    "    #series: DataFrame\n",
    "        #dataframe con columnas de series code, topicos, subtopicos (seriesCETS.csv)\n",
    "    #wdi: DataFrame\n",
    "        #dataframe resultado de outlier\n",
    "def joinSeriesAndWdi(series, wdi):\n",
    "    wdi = wdi.set_index(\"year\").T\n",
    "    wdi.index.name = \"Indicator Code\"\n",
    "    wdi = wdi.reset_index()\n",
    "    inner_join = pd.merge(left=series, right=wdi, left_on=\"Series Code\", right_on=\"Indicator Code\", how=\"inner\")\n",
    "    inner_join = inner_join.drop(columns=\"Series Code\").set_index(\"Indicator Code\")\n",
    "    inner_join.index.name = \"Indicator Code\"\n",
    "    return inner_join\n",
    "\n",
    "#series=readFile(\"seriesCETS.csv\")\n",
    "#wdi = readFile(\"outlier.xlsx\", \"BRA\")\n",
    "#saveFile(joinSeriesAndWdi(series,wdi),\"joinSeriesAndWdi.xlsx\", sheet='BRA')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation(dataFrame, plot = False)\n",
    "    #Retorna dataFrame(correlacion de pearson)\n",
    "#Parameters:\n",
    "    #dataFrame: DataFrame\n",
    "    #plot: Bool\n",
    "        #si es true grafica mediante un mapa de calor la correlacion entre indicadores\n",
    "def correlation(dataFrame, plot = False):\n",
    "    corr = dataFrame.corr(method=\"pearson\")\n",
    "    corr.index.name = 'index'\n",
    "    if plot:\n",
    "        mask = np.zeros_like(corr)\n",
    "        mask[np.triu_indices_from(mask)] = True\n",
    "        with sns.axes_style(\"white\"):\n",
    "            plt.figure(figsize=(10,10))\n",
    "            ax = sns.heatmap(corr, mask=mask, vmax=.3, square=True, annot=True)\n",
    "            plt.show()\n",
    "    return corr\n",
    "#============No tocar esta seccion=====================\n",
    "\n",
    "#data=readFile(\"outlier.xlsx\", \"BRA\").set_index(\"year\")\n",
    "#data = correlation(data)\n",
    "#saveFile(data, \"corr.xlsx\", sheet='hoja1')\n",
    "\n",
    "#runCorrelation(nameMultiSheetExcel, out = \"corr_multiple.xlsx\", sheets = [])\n",
    "    #guarda la correlacion en varias hojas de un archivo excel\n",
    "#Parameters:\n",
    "    #nameMultiSheetExcel:String or group\n",
    "        #nombre del archivo excel con multiples hojas #series=readFile(\"joinSeriesAndWdi.xlsx\", sheet=\"BRA\")->splitByColumn(series, by=\"Topic\", out=\"topic_BRA.xlsx\", frame=False)\n",
    "        #o un dataFrame agrupado splitByColumn(series, by=\"Topic\", out=\"topic_BRA.xlsx\", frame=True)\n",
    "    #out:String\n",
    "        #output name con extencion xlsx\n",
    "    #sheets:Array[String]\n",
    "        #Nombre de las hojas en el archivo excel\n",
    "def runCorrelation(nameMultiSheetExcel, out = \"corr_multiple.xlsx\", sheets = []):\n",
    "    if isinstance(nameMultiSheetExcel, str):\n",
    "        xlsx = excelFile(nameMultiSheetExcel)\n",
    "        with pd.ExcelWriter('datos_reales/{}'.format(out)) as writer:\n",
    "            for sheet in sheets:\n",
    "                sector = pd.read_excel(xlsx, sheet_name=\"{0}\".format(sheet)).reset_index(drop=True).set_index('Indicator Code')\n",
    "                #Se establece en 5 puesto que los años en pathMultiSheetExcel empiezan en este punto\n",
    "                corr = correlation(sector.iloc[:,5:].T)\n",
    "                corr.to_excel(writer, sheet_name=sheet, index=True)\n",
    "    else:\n",
    "        with pd.ExcelWriter('datos_reales/{}'.format(out)) as writer:\n",
    "            for key, group in nameMultiSheetExcel:\n",
    "                sector = group.reset_index(drop=True).set_index('Indicator Code')\n",
    "                corr = correlation(sector.iloc[:,5:].T)\n",
    "                corr.to_excel(writer, sheet_name=str(key), index=True)\n",
    "                \n",
    "#series=readFile(\"joinSeriesAndWdi.xlsx\", sheet=\"BRA\")\n",
    "#splitByColumn(series, by=\"Topic\", out=\"topic_BRA.xlsx\")\n",
    "#nameMultiSheetExcel = \"topic_BRA.xlsx\"\n",
    "#runCorrelation(nameMultiSheetExcel, out = \"corr_multiple.xlsx\", sheets = ['Education','Environment', \"Economic Policy & Debt\",\n",
    "#                                                                       \"Financial Sector\", \"Health\", \"Infrastructure\", \n",
    "#                                                                          \"Poverty\", \"Private Sector & Trade\", \"Public Sector\", \"Social Protection & Labor\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getCorreatedVariables(matrixCorrelated, threshold, name=True)\n",
    "    #retorna dataFrame de correlaciones que sean mayores o iguales a threshold (dataFrame=['X','Y','Val'])\n",
    "#Parameters:\n",
    "    #matrixCorrelated: DataFrame\n",
    "        #Matriz de correlacion\n",
    "    #threshold: Float\n",
    "        #umbral minimo fijado\n",
    "    #names: Bool\n",
    "        #si es true retorna ['X'=Indicator,'Y'=Indicator,'Val'=Correlacion] caso contrario ['X'=i,'Y'=j,'Val'=Correlacion]\n",
    "def getCorreatedVariables(matrixCorrelated, threshold, name=True):\n",
    "    ones = np.ones(matrixCorrelated.shape)\n",
    "    above_diag = np.triu(ones, k=1)#get data only above to diag, and the rest push 0 (), considera los datos encima de la diagonal\n",
    "    get_index = np.where(above_diag)\n",
    "    #(nombreIndice,nombreColumn,valor>=threshold)\n",
    "    if name:\n",
    "        variables_correlated = [(matrixCorrelated.index[i],matrixCorrelated.columns[j], matrixCorrelated.iat[i,j]) for i,j in zip(*get_index) if np.abs(matrixCorrelated.iat[i,j])>= threshold]\n",
    "    else:\n",
    "    #(posicionIndice, posicionColumn, valor>=threshold)\n",
    "        variables_correlated = [(i,j, matrixCorrelated.iat[i,j]) for i,j in zip(*get_index) if np.abs(matrixCorrelated.iat[i,j])>= threshold]\n",
    "    data_frame = pd.DataFrame(variables_correlated, columns=['X','Y','Val'])\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numberIndicatorsBySector(group):\n",
    "    return group.agg(Total=(\"Indicator Code\",\"count\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mostInfluentialValues(index)\n",
    "    #retorna 3 dataframes, primero retorna grupos de indicadores que se correlaciona entre ellos en mas de 75%\n",
    "    #segundo retorna el el promedio de la suma de cuadrados de cada indicador sobre los otros indicadores\n",
    "    #ultimo retorna el mas influyente de un grupo y a los que influye\n",
    "#Parameters:\n",
    "    #index:getCorreatedVariables\n",
    "        #data frame resultado de la funcion getCorreatedVariables donde tiene como columnas X Y Val\n",
    "def mostInfluentialValues(index):\n",
    "    copy_index = index.copy().set_index([\"X\",\"Y\"])\n",
    "    copy_index_list = copy_index.index.tolist()\n",
    "    #mean_x = copy_index.groupby(by=\"X\")[\"Val\"].apply(lambda c: np.sqrt(c**2).mean()).sort_values(ascending=False)\n",
    "    group_x = index.copy().groupby(by=\"X\")['Y'].apply(list).to_dict()\n",
    "    group_x = sorted(group_x.items(), key = lambda item : len(item[1]), reverse=False)\n",
    "    grupos = []\n",
    "    for key, val in group_x:\n",
    "        grupo = list(np.concatenate([val,[key]]))\n",
    "        grupo_aux = grupo.copy()\n",
    "        for i in range(len(grupo)-1):\n",
    "            for j in range (i+1,len(grupo)):\n",
    "                if (grupo[i], grupo[j]) in copy_index_list or (grupo[j], grupo[i]) in copy_index_list:\n",
    "                    pass\n",
    "                else:\n",
    "                    if grupo[j] in grupo_aux:\n",
    "                        grupo_aux.remove(grupo[j])\n",
    "        grupos.append(grupo_aux)\n",
    "    i=0\n",
    "    while i!=len(grupos)+1:\n",
    "        for indicadores in grupos[i+1:]:\n",
    "            flag = True\n",
    "            for ind in grupos[i]:\n",
    "                if ind not in indicadores:\n",
    "                    flag=False\n",
    "                    break\n",
    "            if flag:\n",
    "                grupos.pop(i)\n",
    "                grupos= sorted(grupos, key=len)\n",
    "                i =-1\n",
    "                break\n",
    "        i +=1\n",
    "\n",
    "        \n",
    "    ls_val_inf = []\n",
    "    more_inf = {}\n",
    "    for index, val in enumerate(grupos):\n",
    "        grupo = val\n",
    "        inf_val = []\n",
    "        for i in range(len(grupo)):\n",
    "            cont = 0\n",
    "            for j in range (len(grupo)):\n",
    "                if i!=j:\n",
    "                    if (grupo[i], grupo[j]) in copy_index_list:\n",
    "                        cont +=copy_index.loc[(grupo[i], grupo[j])][\"Val\"]**2\n",
    "                    if (grupo[j], grupo[i]) in copy_index_list:\n",
    "                        cont +=copy_index.loc[(grupo[j], grupo[i])][\"Val\"]**2\n",
    "            inf_val.append(cont/(len(grupo)))\n",
    "        ls_val_inf.append(inf_val)\n",
    "        more_inf[grupos[index][inf_val.index(max(inf_val))]]=grupo\n",
    "    return grupos, ls_val_inf, more_inf #, group_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run_mostInf(sectors, inName)\n",
    "    #retorna dos dataframes, donde el primero es todo las variables mas influyentes de todo los sectores de un pais y el segundo son\n",
    "    #los indicadores independientes que no tiene relacion ninguna con otro indicador\n",
    "#Parameters:\n",
    "    #sector: Array[String]\n",
    "        #array con los sectores pertinentes de un pais, cada sector esta ubicado dentroi de una pestaña de excel\n",
    "    #inName: String\n",
    "        #nombre del archivo excel donde se encuentra las matrices de correlacion de cada sector de un pais\n",
    "def run_mostInf(sectors, inName):\n",
    "    xlsx = excelFile(inName)\n",
    "    dict_more_inf = {}\n",
    "    free_ind = []\n",
    "    max_length = 0\n",
    "    all_ind = []\n",
    "    print(sectors)\n",
    "    for sector in sectors:\n",
    "        print(sector)\n",
    "        corr_var = pd.read_excel(xlsx, sheet_name=sector).set_index(\"index\")\n",
    "        corr_columns = list(corr_var.columns)\n",
    "        corr_var = getCorreatedVariables(corr_var,0.75, True)\n",
    "        grupos, _, more_inf = mostInfluentialValues(corr_var)\n",
    "        for grupo in grupos:\n",
    "            all_ind = np.concatenate((all_ind, grupo), axis=None)\n",
    "        all_ind = np.unique(all_ind)\n",
    "        for col in corr_columns:\n",
    "            if col not in all_ind:\n",
    "                free_ind += [col]    \n",
    "        for key, val in more_inf.items():\n",
    "            dict_more_inf[key]=val \n",
    "            if max_length<len(val):\n",
    "                max_length = len(val)\n",
    "    dict_all_inf_dataframe = {}         \n",
    "    for key,val in dict_more_inf.items():\n",
    "        end = [None] * (max_length - len(val))\n",
    "        dict_all_inf_dataframe[key]=np.concatenate([val,end])\n",
    "        \n",
    "    return dict_all_inf_dataframe, free_ind\n",
    "\n",
    "#sectors = ['Education','Environment']\n",
    "#inf, free_ind = run_mostInf(sectors, \"corr_multiple.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finalIndicators(dataFrame, inf, freeInd)\n",
    "    #Retorna los indicadores de estudio para la imputacion\n",
    "#Parameters:\n",
    "    #dataFrame: DataFrame\n",
    "        #Resultado de la funcion outlier\n",
    "    #inf:Dict\n",
    "        #Indicadores mas influyentes, primer resultado de la funcion run_mostInf\n",
    "    #free_ind: list[String]\n",
    "        #Indicadores independientes, segundo resultado de la funcion run_mostInf\n",
    "def finalIndicators(dataFrame, inf, freeInd):\n",
    "    ind = np.concatenate([list(inf.keys()), freeInd])\n",
    "    return dataFrame.loc[:,ind]\n",
    "\n",
    "#data_frame = readFile(\"delIndicatorMayorUmbralZerosOrNan.xlsx\", \"BRA\")\n",
    "#data_frame = finalIndicators(data_frame, inf, free_ind)\n",
    "#data_frame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#runPreprocess(wdi, series, paises, añosPais)\n",
    "    #Realiza todo los pasos de preprocesamiento de los paises especificados, genera dos archivos cod_pais.xlsx y cor_codpais.xlsx\n",
    "#Parameters:\n",
    "    #wdi:DataFrame\n",
    "        #Archivo excel WDI.xlsx\n",
    "    #series:DataFrame\n",
    "        #Archivo excel series WDI_CETS.csv or xlsx\n",
    "    #paises:List[String]\n",
    "        #lista de codigos de paises existentes en WDI\n",
    "    #añosPais:Dict{key_code_pais:[añoInit,añoEnd]}\n",
    "        #diccionario donde el key es el codigo d pais existente en WDI y la lista correspondiente a esa key\n",
    "        #es el año inicial de analisis y el año final de analisis\n",
    "def runPreprocess(wdi, series, paises, añosPais):\n",
    "    if not os.path.exists(path_file):\n",
    "        os.mkdir(path_file)\n",
    "    if not os.path.exists(path_file+\"/splitByColumn.xlsx\"):\n",
    "        splitByColumn(wdi, \"Country Code\")\n",
    "        print(\"splitByColumn.xlsx file made  Successfully in {}\".format(path_file))\n",
    "    del wdi\n",
    "    gc.collect()\n",
    "    for pais in paises:\n",
    "        print(\"Realizando el preprocesamiento para {}...\".format(pais))\n",
    "        with pd.ExcelWriter('datos_reales/{}.xlsx'.format(pais)) as writer:\n",
    "            #corr.to_excel(writer, sheet_name=sheet, index=True)\n",
    "            data=readFile('splitByColumn.xlsx', pais)\n",
    "            miss_year = countMissingDataPerYear(data,añosPais[pais][0],añosPais[pais][1])\n",
    "            miss_year.to_excel(writer, sheet_name=\"faltantes por año\", index=True)\n",
    "            del miss_year\n",
    "            gc.collect()\n",
    "\n",
    "            miss_ind = countMissingDataPerIndicator(data,añosPais[pais][0],añosPais[pais][1])\n",
    "            miss_ind.to_excel(writer, sheet_name=\"faltantes por ind\", index=True)\n",
    "            del miss_ind\n",
    "            gc.collect()\n",
    "\n",
    "            data= dataFrame(data, añosPais[pais][0],añosPais[pais][1])\n",
    "\n",
    "\n",
    "            menor_umbral=delIndicatorMayorUmbralZerosOrNan(data,25)\n",
    "            menor_umbral.to_excel(writer, sheet_name=\"missing menor 25\", index=True)\n",
    "\n",
    "            menor_umbral_copy = menor_umbral.copy().reset_index()\n",
    "            without_change = delIndicatorWithoutChanges(menor_umbral_copy,True)\n",
    "            without_change.to_excel(writer, sheet_name=\"ind sin variacion\", index=True)\n",
    "            del menor_umbral_copy\n",
    "            gc.collect()\n",
    "\n",
    "            without_change = delIndicatorWithoutChanges(menor_umbral.reset_index(),False)\n",
    "\n",
    "            data_no_atipico, data_atipicos = outlier(without_change)\n",
    "            data_atipicos.to_excel(writer, sheet_name=\"ind eliminados en outlier\", index=True)\n",
    "            \n",
    "            series_wdi = joinSeriesAndWdi(series,data_no_atipico.reset_index())\n",
    "            del data_no_atipico\n",
    "            gc.collect()\n",
    "\n",
    "            grupos = splitByColumn(series_wdi.reset_index(), by=\"Topic\", frame=True)\n",
    "            sectores = list(grupos.groups.keys())\n",
    "            runCorrelation(grupos, out = \"corr_{}.xlsx\".format(pais))\n",
    "            \n",
    "            numberIndicatorsBySector(grupos).to_excel(writer, sheet_name=\"Cant Indicadores\", index=True)\n",
    "            del grupos\n",
    "            gc.collect()\n",
    "            \n",
    "            inf, free_ind = run_mostInf(sectores, \"corr_{}.xlsx\".format(pais))\n",
    "            dataframe_inf = pd.DataFrame(inf)\n",
    "            dataframe_inf.columns = dataframe_inf.columns.str.replace(\".\",\".\")\n",
    "            dataframe_inf.to_excel(writer, sheet_name=\"influyentes\", index=False)\n",
    "            del dataframe_inf\n",
    "            gc.collect()\n",
    "            \n",
    "            pd.DataFrame(free_ind).to_excel(writer, sheet_name=\"sin correlacion\", index=True)\n",
    "            \n",
    "            data_no_atipico, _ = outlier(menor_umbral)\n",
    "            final_data_frame = finalIndicators(data_no_atipico, inf, free_ind)\n",
    "            final_data_frame.to_excel(writer, sheet_name=\"influyentes y sin correlacion\", index=True)\n",
    "            del final_data_frame\n",
    "            gc.collect()\n",
    "    if os.path.exists(path_file+\"/splitByColumn.xlsx\"):\n",
    "          os.remove(path_file+\"/splitByColumn.xlsx\")\n",
    "    else:\n",
    "          pass\n",
    "    print(\"Se realizo el preprocesamiento con exito!!!\")\n",
    "\n",
    "#paises = ['ARG','BOL', 'BRA', 'CHL', 'COL', 'ECU','GUY', 'PER', 'PRY','SUR','TTO', 'URY', 'VEN']\n",
    "#paises = ['BRA']\n",
    "#añosPais = {'ARG':[1991,2016],'BOL':[1990,2017], 'BRA':[1990,2017], 'CHL':[2002,2016],\n",
    "#             'COL':[1990,2017], 'ECU':[1993,2017],'GUY':[2000,2015], 'PER':[1990,2017], \n",
    "#             'PRY':[1995,2017],'SUR':[None,None],'TTO':[None,None], 'URY':[2000,2016], 'VEN':[1991,2014]}\n",
    "#wdi= readFile(\"wdi_0.1.xlsx\",\"Data\")\n",
    "#series = readFile(\"seriesCETS.csv\")\n",
    "#runPreprocess(wdi, series, paises, añosPais)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Imputacion de indicadores en R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Normalizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizeData(dataFrame, transpose=False)\n",
    "    #Normaliza un dataframe estableciendo la media en 0 y la desviacion estandar en 1\n",
    "#Parameters:\n",
    "    #dataFrame:DataFrame\n",
    "        #dataframe a normalizacion, donde las columnas son los indicadores\n",
    "    #transpose:\n",
    "        #Si es true transpone el resultado, caso contrario retorna la misma estructura ingresada\n",
    "def normalizeData(dataFrame, transpose=False):\n",
    "    #std_scale = preprocessing.StandardScaler().fit(_dataFrame)\n",
    "    #data_norm = std_scale.transform(_dataFrame)\n",
    "    data_norm = preprocessing.StandardScaler().fit_transform(dataFrame)\n",
    "    \n",
    "    data_norm_col = pd.DataFrame(data_norm, index=dataFrame.index, columns=dataFrame.columns) \n",
    "    dataFrame.update(data_norm_col)\n",
    "    if transpose:\n",
    "        return dataFrame.T\n",
    "    return dataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalize multidata\n",
    "https://github.com/EL-BID/Agregador-de-indicadores/blob/master/docs/Normalizacion.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#para salvar la normalizacion para VEN año=2017, se repite el ultimo valor que es el año 2016(La variable EG.ELC.ACCS.ZS\n",
    "#esta correlacionada con el año, por lo que no se puede imputar)\n",
    "def NormalizeMultiData(_file, _paises):\n",
    "    xlsx = excelFile(_file)\n",
    "    dataFrame = []\n",
    "    suma = 0\n",
    "    dev = 0\n",
    "    #calcula la media\n",
    "    for pais in _paises:\n",
    "        data = pd.read_excel(xlsx, sheet_name=\"{0}\".format(pais)).set_index('year')\n",
    "        dataFrame.append(data)\n",
    "        suma+=data.values\n",
    "    media = suma/len(_paises)\n",
    "    #calcula la desviacion estanda poblacional\n",
    "    for frame in dataFrame:\n",
    "        dev+=np.power(frame.values-media,2)\n",
    "    dev = np.sqrt(dev/len(_paises))\n",
    "    #guarda los datos normalizados\n",
    "    with pd.ExcelWriter('normalizeMultiData.xlsx') as writer:\n",
    "        for i,frame in enumerate(dataFrame):\n",
    "            frame.iloc[:,:] = ((frame.values-media)/dev)*-1\n",
    "            frame.to_excel(writer, sheet_name=str(_paises[i]), index=True)\n",
    "'''\n",
    "paises = ['ARG','BOL', 'BRA', 'CHL', 'COL', 'ECU','GUY', 'PER', 'PRY','SUR','TTO', 'URY', 'VEN']\n",
    "NormalizeMultiData('imputacion.xlsx', paises)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DMMTS\n",
    "Dynamic multi time series aplicando distancia de mahalanobis en multi time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realiza la covarianza de dos Multi time series X, Y\n",
    "#Observacion: Las columnas deben de ser las variables\n",
    "def pooledCov(X,Y):\n",
    "    '''if X.ndim == 1:\n",
    "        X = np.reshape(np.array(X),(1,-1))\n",
    "    if Y.ndim == 1:\n",
    "        Y = np.reshape(np.array(Y),(1,-1))'''\n",
    "    cvX = np.dot(X,X.T)/X.shape[1]\n",
    "    cvY = np.dot(Y,Y.T)/Y.shape[1]\n",
    "    #cvY = np.cov(Y)        \n",
    "    x1 = X.shape[1]\n",
    "    x2 = Y.shape[1]\n",
    "    return (x1/(x1+x2))*cvX + (x2/(x1+x2))*cvY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mahalanobis distance para dos series de tiempo\n",
    "#Observacion: Las columnas deben de ser las variables\n",
    "def mahalanobis(x=None, y=None):\n",
    "    #centramos los datos en su media aritmetica\n",
    "    a = x-np.mean(x,axis=1,keepdims=True)\n",
    "    b = y-np.mean(y,axis=1, keepdims=True)\n",
    "    #aplicamos la covarianza\n",
    "    inv_covmat = np.linalg.inv(pooledCov(a,b))\n",
    "    mean= np.mean(x, axis=1)-np.mean(y, axis=1)\n",
    "    shape = mean.shape[0]\n",
    "    mean = mean.reshape(-1,shape)\n",
    "    return np.sqrt(np.squeeze(np.dot(mean,np.dot(inv_covmat,mean.T))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Busca el camino optimo de la matriz Dynamic time warping (W)\n",
    "def findPath(W):\n",
    "    i, j = W.shape\n",
    "    i=i-1\n",
    "    j=j-1\n",
    "    path = []\n",
    "    while (i, j) != (0, 0):\n",
    "        if i < 0:\n",
    "            i=0\n",
    "        if j < 0:\n",
    "            j=0\n",
    "        path.append((i, j))\n",
    "        i, j = min((i - 1, j), (i, j - 1), (i - 1, j - 1), key = lambda x: W[x[0], x[1]]) \n",
    "    path.append((0,0))\n",
    "    return W[-1, -1], path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determina la similitud entre dos multi time series, utilizando Dinamic time warping y la distancia de mahalanobis\n",
    "#X,Y (matrices): Grupos con multiples series, las columnas son la serie y las filas son las variables\n",
    "def DMMTS(X,Y,window=0):\n",
    "    W  = sys.maxsize * np.ones((X.shape[1],Y.shape[1]))\n",
    "    if X.shape[0]==1 and Y.shape[0]==1:\n",
    "        W[0,0] = np.absolute(X[:,0].reshape(1,-1)-Y[:,0].reshape(1,-1))\n",
    "        x_aux = [np.absolute(X[:,0].reshape(1,-1)-Y[:,i].reshape(1,-1))+W[0,i-1] for i in range(1,Y.shape[1])]\n",
    "        y_aux = [np.absolute(X[:,1].reshape(1,-1)-Y[:,i].reshape(1,-1))+W[i-1,0]  for i in range(1,X.shape[1])]\n",
    "        W[0,1:]=x_aux\n",
    "        W[1:,0]=y_aux\n",
    "        for i in range(1,X.shape[1]):\n",
    "            for j in range(max(1, i - window), min(Y.shape[1], i + window)):\n",
    "                W[i,j] = np.absolute(np.array(X[:,i]).reshape(1,-1)-np.array(Y[:,j]).reshape(1,-1)) + np.amin([W[i-1,j],W[i-1,j-1],W[i,j-1]])\n",
    "    else:\n",
    "        W[0,0] = mahalanobis(X[:,0].reshape(1,-1),Y[:,0].reshape(1,-1))\n",
    "        x_aux = [mahalanobis(X[:,0].reshape(1,-1),Y[:,i].reshape(1,-1))+W[0,i-1] for i in range(1,Y.shape[1])]\n",
    "        y_aux = [mahalanobis(X[:,1].reshape(1,-1),Y[:,i].reshape(1,-1))+W[i-1,0]  for i in range(1,X.shape[1])]\n",
    "        W[0,1:]=x_aux\n",
    "        W[1:,0]=y_aux\n",
    "        for i in range(1,X.shape[1]):\n",
    "            #for j in range(1, Y.shape[1]):\n",
    "            for j in range(max(1, i - window), min(Y.shape[1], i + window)):\n",
    "                W[i,j] = mahalanobis(np.array(X[:,i]).reshape(1,-1),np.array(Y[:,j]).reshape(1,-1)) + np.amin([W[i-1,j],W[i-1,j-1],W[i,j-1]])\n",
    "                #W[i,j] = np.array(X[:,i]).reshape(1,-1)-np.array(Y[:,j]).reshape(1,-1) + np.amin([W[i-1,j],W[i-1,j-1],W[i,j-1]])\n",
    "    costo, path = findPath(W)       \n",
    "    return W, costo, path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafica la ruta optima\n",
    "#W: matriz Dinamic time warping , path: coordenadas de la ruta optima(x,y)\n",
    "def plotPath(W, path):\n",
    "    x,y = zip(*path)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.imshow(W, origin='lower', interpolation='nearest')\n",
    "    plt.colorbar()\n",
    "    plt.scatter(x,y,color='r')\n",
    "    plt.plot(x, y, color=\"r\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grafica como se cogio las distancias entre indicadores\n",
    "#X,Y: Grupo con multi time series, las filas son las variables y las columnas las series\n",
    "#path: coordenadas de la ruta optima(x,y)\n",
    "def plotDistance(X,Y, path):\n",
    "    offset = 5\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xlim([-1, max(len(X.T), len(Y.T)) + 1])\n",
    "    plt.plot(X.T)\n",
    "    plt.plot(Y.T + offset)\n",
    "    for (x1, x2) in path:\n",
    "        plt.plot([x1, x2], [X.T[x1], Y.T[x2] + offset])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejecucion del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import dendrogram, ward,average\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.spatial.distance import pdist\n",
    "from scipy.spatial.distance import mahalanobis as mh\n",
    "from collections import defaultdict\n",
    "from matplotlib.colors import rgb2hex, colorConverter\n",
    "from scipy.cluster import hierarchy\n",
    "#Probar si funciona\n",
    "#import pdfkit\n",
    "import statsmodels.api as sm\n",
    "import matplotlib as mpl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clusters(dict):#convierte cada etiqueta \"color hexadecimal\" en su respectivo color\n",
    "#Parameters:\n",
    "    #pathWkhtml:String\n",
    "        #ruta del programa wkhtmltopdf.exe, si es None solo muestra la tabla y no realiza el proceso de guardado\n",
    "        #OJO: instalar wkhtmltopdf\n",
    "    #OutName:String\n",
    "        #Nombre del archivo pdf de salida, almacena los clusters encontrados del dendrograma en formato de tabla \n",
    "class Clusters(dict):\n",
    "    def _repr_html_(self, pathWkhtml = \"C:\\src\\wkhtmltopdf\\bin\\wkhtmltopdf.exe\", outName=\"out.html\"):\n",
    "        html = '<table style=\"border:\"1\"; font-family:\"Arial\"; font-size:\"16\"; \">'\n",
    "        html += '<tr><th style=\"border: 1px solid black; text-align: center\">Cluster</th>'\n",
    "        html += '<th style=\"border: 1px solid black; text-align: center\">Indicator</th>'\n",
    "        html += '<th style=\"border: 1px solid black; text-align: center\">Total</th></tr>'\n",
    "        for i, c in enumerate(self):\n",
    "            hx = rgb2hex(colorConverter.to_rgb(c))\n",
    "            html += '<tr style=\"0\">' \\\n",
    "            '<td style=\"background-color: {0}; text-align: center' \\\n",
    "                       'border: 0;\">' \\\n",
    "            '<code style=\"background-color: {0};\">'.format(hx)\n",
    "            html += \"Cluster {0}\".format(i+1) + '</code></td>'\n",
    "            html += '<td style=\"border: 1px solid black; text-align: center\"><code>'\n",
    "            html += repr(self[c]) + '</code></td>'\n",
    "            html +='<td style=\"border: 1px solid black; text-align: center\"><code>'\n",
    "            html += str(len(self[c]))+'</code></td></tr>'\n",
    "        html += '</table>'\n",
    "        with open(outName, \"w\") as file:\n",
    "            file.write(html)\n",
    "            #si pdfkit funciona, activar las siguientes lineas\n",
    "        ''' if pathWkhtml is not None:\n",
    "            path_wkthmltopdf = r'{}'.format(pathWkhtml)\n",
    "            config = pdfkit.configuration(wkhtmltopdf=path_wkthmltopdf)\n",
    "            pdfkit.from_string(html, os.path.join(path_file,outName) , configuration=config)\n",
    "        else:\n",
    "            print(\"Instale el programa wkhtmltopdf y estableza la ruta en el modulo Clusters para guardarlo: Ejemplo {}'.format(pathWkhtml)\")\n",
    "        '''\n",
    "        return html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#retorna las etiquetas de cada cluster en funcion de la distancia\n",
    "def get_cluster_classes(den, max_d, label='ivl'):\n",
    "    cluster_idxs = defaultdict(list)\n",
    "    for c, pi, d in zip(den['color_list'], den['icoord'], den['dcoord']):\n",
    "        for leg in pi[1:3]:\n",
    "            i = (leg - 5.0) / 10.0\n",
    "            if abs(i - int(i)) < 1e-5:\n",
    "                y = d[1]\n",
    "                if y <= max_d:\n",
    "                    if i not in cluster_idxs[c]:\n",
    "                        cluster_idxs[c].append(int(i))\n",
    "    cluster_classes = Clusters()\n",
    "    for c, l in cluster_idxs.items():\n",
    "        i_l = [re.sub(\"&\", \"and\", den[label][i]) for i in l]\n",
    "        cluster_classes[c] = i_l\n",
    "\n",
    "    return cluster_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#las graficas anteriores son erroneas por que no se considera la distancia correcta\n",
    "def Elbow(dendrogram, name=\"dist.png\", size=(15,5), posicion=0):\n",
    "    dist =[]\n",
    "    for d in dendrogram['dcoord']:\n",
    "        dist.append(d[1])\n",
    "    dist = sorted(dist,key=float)\n",
    "    xaxis= np.arange(0,len(dist))\n",
    "    plt.figure(figsize=size)\n",
    "    hfont = {'fontname':'Arial'}\n",
    "    font_size = 16\n",
    "    plt.title(\"Optimal Distance\", fontsize=15, **hfont)\n",
    "    plt.xlabel(\"Step\", fontsize=font_size, **hfont)\n",
    "    plt.ylabel(\"Distance\", fontsize=font_size, **hfont)\n",
    "    plt.annotate(\"Optimal Distance %.6g\" % dist[posicion], (posicion, dist[posicion]), xytext=(-25, 20),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "    plt.plot(xaxis,dist, linestyle='dashed', linewidth=2, markersize=12, label=\"Distance\")\n",
    "    plt.axvline(color='red',x=posicion, linestyle='--')\n",
    "    plt.grid(True)\n",
    "    plt.legend(bbox_to_anchor=(0.01, -0.01, 1, 1), loc='upper left', ncol=1, borderaxespad=0)\n",
    "    #plt.title(\"Elbow\")\n",
    "    plt.savefig(os.path.join(path_file,name), bbox_inches = 'tight',pad_inches = 0)\n",
    "    return dist\n",
    "#Elegir el paso correcto hasta el punto de quiebre\n",
    "#EJEMPLO: Para BOL el paso obtenido era 0.1... pero de acuerdo al analisis se decide colocar en el punto de quiebre \"260\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#realiza el dendrograma con las distancias obteninas\n",
    "#el metodo de agrupacion utilizada es ward\n",
    "def fancy_dendrogram(*args, **kwargs):\n",
    "    max_d = kwargs.pop('max_d', 0)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "    size = kwargs.pop('size', (15,10))\n",
    "    plt.figure(figsize=size)\n",
    "    hfont = {'fontname':'Arial'}\n",
    "    font_size = 16\n",
    "    no_plot_table = kwargs.pop('no_plot_table', None)\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "    clusters = None\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram With Ward Method', fontsize=font_size, **hfont)\n",
    "        plt.xlabel('Sample Index or (cluster size)', fontsize=font_size, **hfont)\n",
    "        plt.ylabel('Distance', fontsize=font_size, **hfont)\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k')\n",
    "        plt.savefig(os.path.join(path_file,\"dendrograma.png\"), bbox_inches = 'tight',pad_inches = 0)\n",
    "        plt.show()\n",
    "    if not no_plot_table: \n",
    "        clusters = get_cluster_classes(ddata, max_d, label='ivl')\n",
    "    return ddata, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Indicadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#procesa un dataframe mediante DMMTS\n",
    "#_dataFrame: dataframe, donde tiene como columnas los indicadores, y filas las series temporales\n",
    "#skeep: numero de indicadores a agrupar\n",
    "#windows:tamaño de la ventana para DMMTS\n",
    "#Observacion:Verificar que no exista # N/A\n",
    "def DataFrameForClustering(_dataFrame, skeep=2, windows=10):\n",
    "    columnas =_dataFrame.shape[1]\n",
    "    costos = []\n",
    "    if columnas%skeep!=0:\n",
    "        raise Exception('El numero de columnas {} debe ser multiplo de las agrupaciones {}'.format(columnas, skeep))\n",
    "    else:\n",
    "        for i in range(0,columnas,skeep):\n",
    "            p1 = _dataFrame.iloc[:,i:i+skeep]\n",
    "            for j in range(i+skeep,columnas,skeep):\n",
    "                p2 = _dataFrame.iloc[:,j:j+skeep]\n",
    "                _,costo,_= DMMTS(p1.values.T,p2.values.T,windows)\n",
    "                costos.append(costo)\n",
    "    return costos\n",
    "# sentencias para iniciar la normalizacion\n",
    "file = readFile('ARG_SECTORES.xlsx','economic policy & debt').set_index('year')\n",
    "file = normalizeData(file)\n",
    "#inicia el calculo de las distancias por indicador de pais\n",
    "#DataFrameForClustering(file,1) el uno inidica el rango de compracion, en este caso es de uno a uno\n",
    "distancias_similitud = DataFrameForClustering(file,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIAR numero_cluster\n",
    "numero_cluster = 13\n",
    "cmap = plt.cm.rainbow(np.linspace(0, 1, numero_cluster)) #Variar de acuerdo al numero de clusters que se obtiene\n",
    "hierarchy.set_link_color_palette([mpl.colors.rgb2hex(rgb[:3]) for rgb in cmap])\n",
    "#METODO WARD\n",
    "linked_array = ward(np.array(distancias_similitud))\n",
    "dn, clusters = fancy_dendrogram(\n",
    "    linked_array,\n",
    "    truncate_mode='lastp',\n",
    "    p=400,\n",
    "    #activar para reporte de indicadores por pais\n",
    "    labels=[i for i in file.columns],#cambiar el arreglo para cambiar las etiquetas del cluster\n",
    "    no_plot=False, #plot dendrogram\n",
    "    no_plot_table =False,#grafica la tabla de clusters\n",
    "    orientation='top',\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=9.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=1,\n",
    "    size=(25,10), #tamaño (width, height) tamaño de configuracion para ind=75,10  sec=15,10, det_sec=25,10\n",
    "    #Activar despues de ejecutar el metodo Elbow con el Step configurado, ejemplo BOL=260\n",
    "    max_d=dist[140]+0.1 #distancia maxima para determinar el numero de cluster \n",
    ")\n",
    "#Observacion: Los archivos generados se remplaza en cada ejecucion\n",
    "#funcion elbow\n",
    "dist=Elbow(dn, size=(15,5), posicion=140)\n",
    "#Instalar el programa del modulo Clusters(dict) y configurar el parametro del modulo con la ruta donde se ha instalado\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot ind\n",
    "media de indicadores, mini graficos de tendencias, tabla resumen de clusters por año y clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotAllClustersIndicators(dataFrame, clusters, cluster_title): genera media de indicadores, mini graficos de tendencias, tabla resumen de clusters por año y clusters.\n",
    "#Parameters:\n",
    "    #dataFrame:DataFrame\n",
    "        #dataframe a analizar\n",
    "    #clusters: Array[String]\n",
    "        #array de indicadores\n",
    "    #cluster_title: String\n",
    "        #Titulo del grafico\n",
    "def plotAllClustersIndicators(dataFrame, clusters, cluster_title):\n",
    "    font_size = 16\n",
    "    hfont = {'fontname':'Arial'}\n",
    "    data_frame = dataFrame\n",
    "    plt.figure(figsize=(15,5))\n",
    "    plt.subplot()\n",
    "    means = {}\n",
    "    colores = []\n",
    "    plt.xlabel(\"Year\", fontsize=font_size, **hfont)\n",
    "    plt.ylabel(\"Normalized Value\", fontsize=font_size, **hfont)\n",
    "    i = 1\n",
    "    for key, val in clusters.items():\n",
    "        mean_cluster = data_frame.loc[:,val].mean(axis=1)\n",
    "        name = \"Cluster \"+ str(i)\n",
    "        means[name] = mean_cluster\n",
    "        p = plt.plot(mean_cluster,label=name, marker='o', linewidth=1.5)\n",
    "        colores.append(p[0].get_color())\n",
    "        i +=1\n",
    "    plt.legend(bbox_to_anchor=(0.0, 0.39, 1, 1), loc='upper center', ncol=7 , mode=\"expand\", borderaxespad=0)\n",
    "    plt.grid(True)   \n",
    "    plt.title(str(cluster_title).title(), fontsize=font_size, **hfont)\n",
    "    plt.savefig(os.path.join(path_file,\"media.png\"), bbox_inches = 'tight',pad_inches = 0)\n",
    "    #Grafica subgraficos del cluster \n",
    "    x0,x1,y0,y1 = plt.axis() #obtiene los puntos iniciales y finales del eje x e y\n",
    "    i = 1\n",
    "    plt.figure(figsize=(15,20))\n",
    "    for key, val in means.items():\n",
    "        x1=plt.subplot(7, 4, i)\n",
    "        plt.ylim(y0, y1)\n",
    "        #plt.title(key.title(),fontsize=12, **hfont)\n",
    "        ciclo, tend = sm.tsa.filters.hpfilter(val, lamb=6.25)#6.25 es para años y 1000 u otros valores para dias, consultar wdi.txt\n",
    "        plt.plot(val, label=key, marker='', linewidth=1.7, color=colores[i-1])#cluster\n",
    "        plt.scatter(list(tend.index),tend,label=\"Tendencia\", marker='.', linewidth=0.3, color=\"gray\")#tendencia\n",
    "        plt.setp(x1.get_xticklabels(), visible=True)\n",
    "        plt.legend(bbox_to_anchor=(0, 0, 1, 1), fontsize=8)#coloca legend inteligentemente sin tapar la grafica\n",
    "        plt.grid(True)\n",
    "        i +=1\n",
    "\n",
    "    plt.subplots_adjust(top=1.0, bottom=0.08, left=0.10, right=0.95, hspace=0.30, wspace=0.15)\n",
    "    plt.savefig(os.path.join(path_file,\"tendencias.png\"), bbox_inches = 'tight',pad_inches = 0)\n",
    "    plt.show()\n",
    "    return means\n",
    "    #plt.savefig('Representativo.png',bbox_inches = 'tight',pad_inches = 0)\n",
    "dataFrame = readFile(\"ARG_SECTORES.xlsx\",\"economic policy & debt\")\n",
    "dataFrame = normalizeData(dataFrame.set_index(\"year\"))\n",
    "cluster_title = \"Economic policy and debt from Argentina\"\n",
    "mean = plotAllClustersIndicators(dataFrame, clusters, cluster_title)\n",
    "saveFile(pd.DataFrame(mean).T.describe().T, \"describe_cluster_anio.xlsx\")\n",
    "saveFile(pd.DataFrame(mean).describe().T, \"describe_cluster.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reporte de sectores en un archivo excel, cuando join es True retorna en unico archivo excel los sectores. <br>\n",
    "Observacion: Reporte unicamente para indicadores por pais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unir(x)\n",
    "    #funcion usada en agg de clustersDescripcion, antepone una coma a un string \n",
    "#Parameters:\n",
    "    #x:String\n",
    "        #String a anteponer la coma\n",
    "def unir(x):\n",
    "    return \", \".join(x)\n",
    "\n",
    "def totalInd(inf, indInf):\n",
    "    aux = indInf.copy()\n",
    "    contador = 0\n",
    "    for ind in indInf:\n",
    "        if ind not in inf.columns:\n",
    "            aux.remove(ind)\n",
    "            contador +=1\n",
    "    if len(aux)==0:\n",
    "        return contador\n",
    "    inf = np.unique(inf.loc[:,aux].values.tolist()).shape[0]-1\n",
    "    return inf+contador\n",
    "\n",
    "#clustersDescripcion(series, cluster, by=\"Topic\")\n",
    "    #Recupera los sectores de los indicadores que forman un cluster, los sectores se obtienen de series\n",
    "#Parameters:\n",
    "    #series: DataFrame\n",
    "        #dataframe del archivo seriesCets.csv\n",
    "    #cluster:List(String)\n",
    "        #Lista de todo los indicadores que forman un cluster\n",
    "    #total:DataFrame\n",
    "        #dataframe que contiene la cantidad de indicadores influyentes de un sector de un determinado pais[index(topico), column=Nro]\n",
    "    #by:String\n",
    "        #Columna de seriesCets mediante el cual agrupar\n",
    "\n",
    "def clustersDescripcion(series, cluster,total=None, inf=None, by=\"Topic\"):\n",
    "    series = series.set_index(\"Series Code\").T\n",
    "    if total is None and inf is None:\n",
    "        return series.loc[[by],cluster].T.reset_index().groupby(by=by).agg(Cod_ind=('Series Code',unir),\n",
    "                                                                       Nro=(\"Series Code\",'count'))\n",
    "    else:\n",
    "        mezcla = pd.merge(series.loc[[by],cluster].T.reset_index().groupby(by=by).agg(Cod_ind=('Series Code',unir),\n",
    "                                                                      Nro=(\"Series Code\",'count')), total, on=by, how=\"inner\")\n",
    "        mezcla[\"%\"] = round((mezcla[\"Nro_x\"]*100)/mezcla[\"Nro_y\"],2)\n",
    "        mezcla.columns = [\"Indicadores influyentes e independientes (Inf)\",\"# A\",\"# A por sector\",\"% B\"]\n",
    "        #A:total de influyentes mas independientes\n",
    "        #B:el porcentaje de A sobre #A por sector\n",
    "        total_inds_por_grupo = []\n",
    "        for inds in mezcla.loc[:,\"Indicadores influyentes e independientes (Inf)\"]:\n",
    "            l = list(map(lambda x:x.strip(), inds.split(\",\")))\n",
    "            total_inds_por_grupo.append(totalInd(inf,l))\n",
    "        mezcla[\"C\"] = total_inds_por_grupo ## numero total de influyentes, influidos e independientes\n",
    "        mezcla.sort_index(inplace=True)\n",
    "        return mezcla\n",
    "#nro = 1\n",
    "#cluster = clusters[list(clusters.keys())[nro-1]]\n",
    "#series = readFile(\"seriesCets.csv\")\n",
    "#series = clustersDescripcion(series, cluster)\n",
    "\n",
    "#runClustersDescripcion(series, clusters, outName)\n",
    "    #Guarda el reporte de todo los clusters de un pais\n",
    "#Parameters:\n",
    "    #series:DataFrame\n",
    "        #dataframe del archivo seriesCets.csv\n",
    "    #inf:DataFrame\n",
    "        #dataframe del archivo influyentes de un pais especifico a ser analizado\n",
    "    #clusters:dict\n",
    "        #diccionario con los clusters de un pais, se obtiene del reporte del dendrograma\n",
    "    #outName:String\n",
    "        #nombre del reporte resultante\n",
    "    #join: Boolean\n",
    "        #si es true guarda el reporte de cluster por sector agrupado en un excel, caso contrario cada sector en pestañas diferentes\n",
    "def runClustersDescripcion(series, inf, clusters, outName, join):\n",
    "    with pd.ExcelWriter('datos_reales/{}.xlsx'.format(outName)) as writer:\n",
    "        values = []\n",
    "        for key, val in clusters.items():\n",
    "            values +=val\n",
    "        total=clustersDescripcion(series, values).iloc[:,1]\n",
    "        if join:\n",
    "            mini_col = [\"A\",\"% B\",\"C\"]\n",
    "            tuple_columns = [(sector,m) for sector in total.index for m in mini_col]\n",
    "            cols = pd.MultiIndex.from_tuples(tuple_columns)\n",
    "            join_data_frame = pd.DataFrame(columns=cols, index=np.arange(1,len(clusters)+1))\n",
    "            i = 1\n",
    "            for key, val in clusters.items():\n",
    "                resumen = clustersDescripcion(series, list(val), total, inf).iloc[:,[1,3,4]].T\n",
    "                for column in resumen.columns:\n",
    "                    join_data_frame.loc[i,column]=list(resumen[column])\n",
    "                i +=1\n",
    "            totales = join_data_frame.sum(axis=0)\n",
    "            join_data_frame.loc[\"Total\"]= totales\n",
    "            join_data_frame.index.name = \"Clusters\"\n",
    "            join_data_frame.to_excel(writer, sheet_name=\"{}\".format(outName[:3]), index=True)\n",
    "        else:\n",
    "            i = 1\n",
    "            for key, val in clusters.items():\n",
    "                clustersDescripcion(series, list(val), total, inf).to_excel(writer, sheet_name=\"Cluster{}\".format(str(i)), index=True)\n",
    "                i +=1\n",
    "                \n",
    "series = readFile(\"seriesCets.csv\")\n",
    "influyentes = readFile(\"ARG.xlsx\",\"influyentes\")\n",
    "#outName: cambiar de acuerdo al pais analizado\n",
    "runClustersDescripcion(series, influyentes, clusters, outName=\"ARG_clusters_join\", join=True)\n",
    "runClustersDescripcion(series, influyentes, clusters, outName=\"ARG_clusters\", join=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grafica un cluster especifico de indicadores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion: grafica un cluster del reporte indicador por pais\n",
    "#urlFile: archivo excel de un pais separado por sectores\n",
    "#sheet_name: nombre de hoja del archivo excel\n",
    "#_namesInClusters: array de string con los nombres de los indicadores\n",
    "#title: titulo del grafico\n",
    "#colEtiquetas: numero de columnas de la leyenda\n",
    "#normalizacion: si es True normaliza los datos\n",
    "#opcion: si es 0 grafica la media del cluster, caso contrario grafica los indicadores del cluster\n",
    "def plotClustersIndicators(urlFile, sheet_name, _namesInClusters, title, colEtiquetas, normalizacion=True, opcion = 0):\n",
    "    xlsx = excelFile(urlFile)\n",
    "    font_size = 16\n",
    "    hfont = {'fontname':'Arial'}\n",
    "    if normalizacion:\n",
    "        indicadores=normalizeData(pd.read_excel(xlsx, sheet_name=sheet_name).set_index(\"year\").loc[:,_namesInClusters])\n",
    "    else:\n",
    "        indicadores=pd.read_excel(xlsx, sheet_name=sheet_name).set_index(\"year\").loc[:,_namesInClusters]\n",
    "    vmin = math.floor(indicadores.min().min())\n",
    "    vmax = math.ceil(indicadores.max().max())\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.subplot()\n",
    "    plt.ylim(vmin, vmax)\n",
    "    if opcion ==0:\n",
    "        plt.xlabel(\"Year\", fontsize=font_size, **hfont)\n",
    "        plt.ylabel(\"Normalized Value\", fontsize=font_size, **hfont)\n",
    "        for i in indicadores:\n",
    "            plt.plot(indicadores.loc[:,i],label=str(i), marker='', linewidth=1.5)\n",
    "        col = math.ceil(len(indicadores)/colEtiquetas)\n",
    "        plt.legend(bbox_to_anchor=(0.0, 0.29, 1, 1), loc='upper center', ncol=math.ceil(len(indicadores)/col), mode=\"expand\", borderaxespad=0)\n",
    "    if opcion ==1:\n",
    "        plt.xlabel(\"Year\", fontsize=font_size, **hfont)\n",
    "        plt.ylabel(\"Average Normalized Value\", fontsize=font_size, **hfont)\n",
    "        plt.plot(indicadores.mean(axis=1),label=\"media\", marker='', linewidth=1.5)   \n",
    "        plt.legend(bbox_to_anchor=(0.01, -0.01, 1, 1), loc='upper left', ncol=1, borderaxespad=0)\n",
    "    plt.grid(True)   \n",
    "    plt.title((\"Cluster \"+str(title)).title(), fontsize=font_size, **hfont)\n",
    "    plt.show()\n",
    "    #plt.savefig('Representativo.png',bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "    #return indicadores\n",
    "urlFile = 'ARG_SECTORES.xlsx'\n",
    "sheet_name = 'economic policy & debt'\n",
    "cluster = 12\n",
    "_namesInClusters = clusters[list(clusters.keys())[cluster-1]]\n",
    "colEtiquetas = 5\n",
    "normalizacion = True\n",
    "plotClustersIndicators(urlFile, sheet_name, _namesInClusters, cluster, colEtiquetas, normalizacion, opcion = 0)\n",
    "plotClustersIndicators(urlFile, sheet_name, _namesInClusters, cluster, colEtiquetas, normalizacion, opcion = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitByTopicNormalizeData(series, normalize, outName)\n",
    "    #Une los archivos serieCets y el resultado de normalizar, luego se agrupa por grupos(Topic) y se guarda en un archivo excel \n",
    "#Parameters:\n",
    "    #series:DataFrame\n",
    "        #dataframe del archivo seriesCets.csv\n",
    "    #dataFrame:DataFrame\n",
    "        #data frame a analizar\n",
    "    #outName:String\n",
    "        #Nombre del archivo resultado\n",
    "def splitByTopicNormalizeData(series, dataFrame, outName):\n",
    "    union = joinSeriesAndWdi(series, dataFrame)\n",
    "    with pd.ExcelWriter('datos_reales/{}.xlsx'.format(outName)) as writer:\n",
    "        for key, group in union.groupby(by=\"Topic\"):\n",
    "            group = group.iloc[:,5:].T\n",
    "            group.index.name = \"year\"\n",
    "            group.to_excel(writer, sheet_name=str(key).lower(), index=True)\n",
    "    print(\"Succesfully complete!!!!\")\n",
    "wdi = readFile(\"BOL.xlsx\",\"imp\")\n",
    "wdi = normalizeData(wdi)\n",
    "series = readFile(\"seriesCets.csv\")\n",
    "splitByTopicNormalizeData(series, wdi, outName=\"BOL_SECTORES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#procesa un excel mediante DMMTS, solo para sectores por pais\n",
    "#sectores: Lista de sheet en el archivo excel \n",
    "def ExcelDataForClustering(urlFile, sectores):\n",
    "    costos = []\n",
    "    matrix = np.zeros(())\n",
    "    xlsx = excelFile(urlFile)\n",
    "    for i,data in enumerate(sectores):\n",
    "        p1 = pd.read_excel(xlsx, sheet_name=\"{0}\".format(data)).set_index(\"year\")\n",
    "        for j in range(i+1, len(sectores)):\n",
    "            p2 = pd.read_excel(xlsx, sheet_name=\"{0}\".format(sectores[j])).set_index(\"year\")\n",
    "            _,costo,_= DMMTS(p1.values.T,p2.values.T,5)\n",
    "            costos.append(costo)\n",
    "    return costos\n",
    "\n",
    "\n",
    "#retorna los sectores con datos\n",
    "#urlFile: ingresar nombre del archivo excel estudiado dividido por sectores del modulo anterior (splitByTopicNormalizeData)\n",
    "def buscarSectores(urlFile):\n",
    "    sectores = []\n",
    "    xlsx = excelFile(urlFile)\n",
    "    for data in xlsx.sheet_names:\n",
    "        if len(pd.read_excel(xlsx, sheet_name=\"{0}\".format(data)).columns[1:])>0:\n",
    "            sectores.append(data)\n",
    "    return sectores\n",
    "\n",
    "urFile = 'BOL_SECTORES.xlsx'\n",
    "sectores = buscarSectores(urFile)\n",
    "distancia_sectores = ExcelDataForClustering(urFile,sectores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#VARIAR numero_cluster\n",
    "numero_cluster = 2\n",
    "cmap = plt.cm.rainbow(np.linspace(0, 1, numero_cluster)) #Variar de acuerdo al numero de clusters que se obtiene\n",
    "hierarchy.set_link_color_palette([mpl.colors.rgb2hex(rgb[:3]) for rgb in cmap])\n",
    "linked_array = ward(np.array(distancia_sectores))\n",
    "#z = hierarchy.cut_tree(linked_array, n_clusters=12)\n",
    "dn, clusters = fancy_dendrogram(\n",
    "    linked_array,\n",
    "    truncate_mode='lastp',\n",
    "    p=400,\n",
    "    labels=[re.sub(\"&\",\"and\", i).title() for i in sectores],#cambiar el arreglo para cambiar las etiquetas del cluster\n",
    "    no_plot=False, #plot dendrogram\n",
    "    no_plot_table =False,#grafica la tabla de clusters\n",
    "    orientation='top',\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=9.,\n",
    "    show_contracted=True,\n",
    "    annotate_above=1,\n",
    "    size=(15,10), #tamaño (width, height)\n",
    "    #Activar despues de ejecutar el metodo Elbow con el Step configurado, ejemplo BOL=260\n",
    "    max_d=dist[6]+0.1 #distancia maxima para determinar el numero de cluster \n",
    ")\n",
    "#Observacion: Los archivos generados se remplaza en cada ejecucion\n",
    "#funcion elbow\n",
    "dist=Elbow(dn, size=(15,5), posicion=6)\n",
    "#Instalar el programa del modulo Clusters(dict) y configurar el parametro del modulo con la ruta donde se ha instalado\n",
    "clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### plot sectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion: grafica todo los indicadores pertenecientes a los sectores de un cluster \n",
    "#urlFile: archivo excel de un pais separado por sectores\n",
    "#_namesInClusters: array de string con los nombres de los sectores a graficar\n",
    "#colEtiquetas: las etiquetas de cada serie en cuantas columnas se desea agrupar\n",
    "#opcion: 0:grafica el cluster, 1: grafica la media del cluster\n",
    "def plotClustersSector(urlFile, _namesInClusters, colEtiquetas, opcion = 0):\n",
    "    xlsx = excelFile(urlFile)\n",
    "    sectores = pd.read_excel(xlsx, sheet_name=\"{0}\".format(_namesInClusters[0])).set_index(\"year\")\n",
    "    for data in range(1,len(_namesInClusters)):\n",
    "        aux=pd.read_excel(xlsx, sheet_name=\"{0}\".format(_namesInClusters[data])).set_index(\"year\")\n",
    "        sectores = pd.concat([sectores,aux],axis=1)\n",
    "    plt.figure(figsize=(14,5))\n",
    "    vmin = math.floor(sectores.min().min())\n",
    "    vmax = math.ceil(sectores.max().max())\n",
    "    hfont = {'fontname':'Arial'}\n",
    "    font_size = 16\n",
    "    if opcion == 0:\n",
    "        plt.xlabel(\"Year\", fontsize=font_size, **hfont)\n",
    "        plt.ylabel(\"Normalized Value\", fontsize=font_size, **hfont)\n",
    "        for i in sectores:\n",
    "            plt.plot(sectores.loc[:,i],label=str(i), marker='', linewidth=1.5)\n",
    "        col = math.ceil(len(sectores)/colEtiquetas)\n",
    "        plt.legend(bbox_to_anchor=(0.0, 0.29, 1, 1), loc='upper center', ncol=math.ceil(len(sectores)/col), mode=\"expand\", borderaxespad=0)\n",
    "    if opcion == 1:\n",
    "        plt.xlabel(\"Year\", fontsize=font_size, **hfont)\n",
    "        plt.ylabel(\"Average Normalized Value\", fontsize=font_size, **hfont)\n",
    "        vmin = math.floor(sectores.mean(axis=1).min().min())\n",
    "        vmax = math.ceil(sectores.mean(axis=1).max().max())\n",
    "        plt.plot(sectores.mean(axis=1),label=\"media\", marker='', linewidth=1.5)   \n",
    "        plt.legend(bbox_to_anchor=(0.01, -0.01, 1, 1), loc='upper left', ncol=1, borderaxespad=0)\n",
    "    #plt.plot(sectores.mean(axis=1),label=\"media\", marker='', linewidth=2.5) \n",
    "    plt.ylim(vmin, vmax)\n",
    "    plt.grid(True)\n",
    "    plt.title(re.sub(\"&\", \"and\", \", \".join(_namesInClusters)).title() + \" - Cluster\", fontsize=font_size, **hfont)\n",
    "    plt.plot()\n",
    "    #plt.savefig('Representativo.png',bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "    #return sectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Funcion: grafica todo los indicadores pertenecientes a un solo sector del mismo cluster anterior\n",
    "#urlFile: archivo excel de un pais separado por sectores\n",
    "#_namesInClusters: array de string con los nombres de los sectores a graficar\n",
    "#colEtiquetas: las etiquetas de cada serie en cuantas columnas se desea agrupar\n",
    "#opcion: 0:grafica el sector, 1: grafica la media del sector\n",
    "#ymin = rango minimo en el eje Y para graficar\n",
    "#ymax = rango maximo en el eje Y para graficar\n",
    "def plotClustersSector1(urlFile, _namesInClusters, colEtiquetas, opcion = 0, ymin=-2, ymax=2):\n",
    "    xlsx = excelFile(urlFile)\n",
    "    font_size = 16\n",
    "    hfont = {'fontname':'Arial'}\n",
    "    for data in range(0,len(_namesInClusters)):\n",
    "        sectores=pd.read_excel(xlsx, sheet_name=\"{0}\".format(_namesInClusters[data])).set_index(\"year\")\n",
    "        plt.figure(figsize=(14,5))\n",
    "        plt.subplot()\n",
    "        plt.ylim(ymin, ymax)\n",
    "        if opcion ==0:\n",
    "            plt.xlabel(\"Year\", fontsize=font_size, **hfont)\n",
    "            plt.ylabel(\"Normalized Value\", fontsize=font_size, **hfont)\n",
    "            for i in sectores:\n",
    "                plt.plot(sectores.loc[:,i],label=str(i), marker='', linewidth=1.5)\n",
    "            col = math.ceil(len(sectores)/colEtiquetas)\n",
    "            plt.legend(bbox_to_anchor=(0.0, 0.29, 1, 1), loc='upper center', ncol=math.ceil(len(sectores)/col), mode=\"expand\", borderaxespad=0)\n",
    "        if opcion ==1:\n",
    "            plt.xlabel(\"Year\", fontsize=font_size, **hfont)\n",
    "            plt.ylabel(\"Average Normalized Value\", fontsize=font_size, **hfont)\n",
    "            plt.plot(sectores.mean(axis=1),label=\"media\", marker='', linewidth=1.5)   \n",
    "            plt.legend(bbox_to_anchor=(0.01, -0.01, 1, 1), loc='upper left', ncol=1, borderaxespad=0)\n",
    "        plt.grid(True)   \n",
    "        plt.title(re.sub(\"sector\", \"\", re.sub(\"&\", \"and\", _namesInClusters[data])).title() + \" Sector\", fontsize=font_size, **hfont)\n",
    "    plt.show()\n",
    "    #plt.savefig('Representativo.png',bbox_inches = 'tight',pad_inches = 0)\n",
    "\n",
    "    #return sectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urFile = 'BOL_SECTORES.xlsx'\n",
    "cluster = 1\n",
    "_namesInClusters = [re.sub(\"and\", \"&\",i.lower()) for i in clusters[list(clusters.keys())[cluster-1]]]\n",
    "#_namesInClusters = [re.sub(\"and\", \"&\",i.lower()) for i in ['health']]\n",
    "colEtiquetas = 12\n",
    "data = plotClustersSector(urFile,_namesInClusters,colEtiquetas, opcion =0)\n",
    "#ymin, ymax: parametros de escala\n",
    "data1 = plotClustersSector1(urFile,_namesInClusters,colEtiquetas, opcion=0, ymin=-3, ymax=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descomposicion de una serie de indicadores\n",
    "Explicacion de la ejecucion de la variable <strong>_namesInClusters</strong> usando un cluster<br>\n",
    "1) Obtener los keys del diccionario<br>\n",
    " $keys=list(clusters.keys())$<br>\n",
    "2) Seleccionamos una posicion de la lista de claves obtenidas en el paso 1 <br>\n",
    " $key = keys[1]$<br>\n",
    "3) recuperamos los valores correspondientes a la clave del paso 2<br>\n",
    "$clusters[key]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotRollingAverage(urlFile, sheet_name, _namesInClusters, title, colEtiquetas, normalizacion=True, opcion = 0, freq=5):\n",
    "    xlsx = excelFile(urlFile)\n",
    "    font_size = 16\n",
    "    hfont = {'fontname':'Arial'}\n",
    "    if normalizacion:\n",
    "        indicadores=normalizeData(pd.read_excel(xlsx, sheet_name=sheet_name).set_index(\"year\").loc[:,_namesInClusters]).rolling(freq).mean()\n",
    "    else:\n",
    "        indicadores=pd.read_excel(xlsx, sheet_name=sheet_name).set_index(\"year\").loc[:,_namesInClusters].rolling(freq).mean()\n",
    "    vmin = math.floor(indicadores.min().min())\n",
    "    vmax = math.ceil(indicadores.max().max())\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.subplot()\n",
    "    plt.ylim(vmin, vmax)\n",
    "    if opcion ==0:\n",
    "        plt.xlabel(\"Year\", fontsize=font_size, **hfont)\n",
    "        plt.ylabel(\"Normalized Value\", fontsize=font_size, **hfont)\n",
    "        for i in indicadores:\n",
    "            plt.plot(indicadores.loc[:,i],label=str(i), marker='', linewidth=1.5)\n",
    "        col = math.ceil(len(indicadores)/colEtiquetas)\n",
    "        plt.legend(bbox_to_anchor=(0.0, 0.29, 1, 1), loc='upper center', ncol=math.ceil(len(indicadores)/col), mode=\"expand\", borderaxespad=0)\n",
    "    if opcion ==1:\n",
    "        plt.xlabel(\"Year\", fontsize=font_size, **hfont)\n",
    "        plt.ylabel(\"Average Normalized Value\", fontsize=font_size, **hfont)\n",
    "        plt.plot(indicadores.mean(axis=1),label=\"media\", marker='', linewidth=1.5)   \n",
    "        plt.legend(bbox_to_anchor=(0.01, -0.01, 1, 1), loc='upper left', ncol=1, borderaxespad=0)\n",
    "    plt.grid(True)   \n",
    "    plt.title((\"Rolling Average \"+str(title)).title(), fontsize=font_size, **hfont)\n",
    "    plt.show()\n",
    "\n",
    "###########################rolling Avergae #################################\n",
    "urlFile = 'BOL.xlsx'\n",
    "sheet_name = 'imp'\n",
    "\n",
    "#ingresar el cluster a analizar\n",
    "cluster = 0 \n",
    "#_namesInClusters = clusters[list(clusters.keys())[cluster-1]]\n",
    "\n",
    "#Arreglo de indicadores si no se utiliza cluster\n",
    "_namesInClusters = [\"NY.TRF.NCTR.KN\",\"NE.CON.GOVT.KD.ZG\"]\n",
    "\n",
    "colEtiquetas = 5\n",
    "normalizacion=True\n",
    "#opcion = 0: rolling averange de los indicadores, opcion=1: rolling average del promedio\n",
    "plotRollingAverage(urlFile, sheet_name, _namesInClusters, cluster, colEtiquetas, normalizacion, opcion = 0, freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotTendencyAndCicle(urlFile, sheet_name, _namesInClusters, title, colEtiquetas, normalizacion=True, opcion = 0):\n",
    "    xlsx = excelFile(urlFile)\n",
    "    font_size = 16\n",
    "    hfont = {'fontname':'Arial'}\n",
    "    if normalizacion:\n",
    "        indicadores=normalizeData(pd.read_excel(xlsx, sheet_name=sheet_name).set_index(\"year\").loc[:,_namesInClusters])\n",
    "    else:\n",
    "        indicadores=pd.read_excel(xlsx, sheet_name=sheet_name).set_index(\"year\").loc[:,_namesInClusters]\n",
    "    ciclo, tend = sm.tsa.filters.hpfilter(indicadores, lamb=6.25)\n",
    "    plt.figure(figsize=(14,5))\n",
    "    plt.subplot()\n",
    "    desc = None\n",
    "    if opcion ==0:\n",
    "        desc = tend\n",
    "        plt.title((\"Trend\").title(), fontsize=font_size, **hfont)\n",
    "    if opcion ==1:\n",
    "        desc = ciclo\n",
    "        plt.title((\"Cycle\").title(), fontsize=font_size, **hfont)\n",
    "\n",
    "        \n",
    "    plt.xlabel(\"Year\", fontsize=font_size, **hfont)\n",
    "    plt.ylabel(\"Normalized Value\", fontsize=font_size, **hfont)\n",
    "    vmin = math.floor(desc.min().min())\n",
    "    vmax = math.ceil(desc.max().max())\n",
    "    plt.ylim(vmin, vmax)\n",
    "    for i in desc:\n",
    "        plt.plot(desc.loc[:,i],label=str(i), marker='', linewidth=1.5)\n",
    "    col = math.ceil(len(desc)/colEtiquetas)\n",
    "    plt.legend(bbox_to_anchor=(0.0, 0.29, 1, 1), loc='upper center', ncol=math.ceil(len(desc)/col), mode=\"expand\", borderaxespad=0)\n",
    "    plt.grid(True)   \n",
    "    plt.show()\n",
    "\n",
    "#######################tendencia y ciclo ##########################################\n",
    "urlFile = 'BOL.xlsx'\n",
    "sheet_name = 'imp'\n",
    "\n",
    "#ingresar el cluster a analizar\n",
    "cluster = 0 \n",
    "#_namesInClusters = clusters[list(clusters.keys())[cluster-1]]\n",
    "\n",
    "#Arreglo de indicadores si no se utiliza cluster\n",
    "_namesInClusters = [\"NY.TRF.NCTR.KN\",\"NE.CON.GOVT.KD.ZG\"]\n",
    "colEtiquetas = 5\n",
    "normalizacion=True\n",
    "plotTendencyAndCicle(urlFile, sheet_name, _namesInClusters, cluster, colEtiquetas, normalizacion, opcion = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Descompocicion de series para un input de indicadores, segun el modelo ARIMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#freq: 5 para años y 1600 para dias\n",
    "def plotDescomposition(urlFile, sheet_name, _namesInClusters, title, colEtiquetas, normalizacion=True, freq=5):\n",
    "    xlsx = excelFile(urlFile)\n",
    "    font_size = 16\n",
    "    hfont = {'fontname':'Arial'}\n",
    "    if normalizacion:\n",
    "        indicadores=normalizeData(pd.read_excel(xlsx, sheet_name=sheet_name).set_index(\"year\").loc[:,_namesInClusters])\n",
    "    else:\n",
    "        indicadores=pd.read_excel(xlsx, sheet_name=sheet_name).set_index(\"year\").loc[:,_namesInClusters]\n",
    "    descomposicion = sm.tsa.seasonal_decompose(indicadores.interpolate(),model='additive', freq=freq, two_sided=False) \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(14,10))\n",
    "    fig.suptitle('Decomposition of Cluster {}'.format(title), fontsize=font_size, **hfont)\n",
    "    \n",
    "    desc = descomposicion.observed\n",
    "    for i in desc:\n",
    "        axs[0, 0].plot(desc.loc[:,i],label=str(i), marker='', linewidth=1.5)\n",
    "    axs[0, 0].set_title((\"Cluster\").title(), fontsize=font_size, **hfont)\n",
    "    col = math.ceil(len(desc)/colEtiquetas)\n",
    "    axs[0, 0].grid(True)\n",
    "    \n",
    "    desc = descomposicion.trend\n",
    "    for i in desc:\n",
    "        axs[0, 1].plot(desc.loc[:,i],label=str(i), marker='', linewidth=1.5)\n",
    "    axs[0, 1].set_title((\"Rolling Average\").title(), fontsize=font_size, **hfont)\n",
    "    col = math.ceil(len(desc)/colEtiquetas)\n",
    "    axs[0, 1].grid(True)\n",
    "    \n",
    "    desc = descomposicion.seasonal\n",
    "    for i in desc:\n",
    "        axs[1, 0].plot(desc.loc[:,i],label=str(i), marker='', linewidth=1.5)\n",
    "    axs[1, 0].set_title((\"Seasonal\").title(), fontsize=font_size, **hfont)\n",
    "    col = math.ceil(len(desc)/colEtiquetas)\n",
    "    axs[1, 0].grid(True)\n",
    "    \n",
    "    desc = descomposicion.resid\n",
    "    for i in desc:\n",
    "        axs[1, 1].plot(desc.loc[:,i],label=str(i), marker='', linewidth=1.5)\n",
    "    axs[1, 1].set_title((\"Residue\").title(), fontsize=font_size, **hfont)\n",
    "    col = math.ceil(len(desc)/colEtiquetas)\n",
    "    axs[1, 1].grid(True)\n",
    "    fig.legend(bbox_to_anchor=(0.0, 0.10, 1, 1), loc='upper center', ncol=math.ceil(len(desc)/col), mode=\"expand\", borderaxespad=0)\n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='Year', ylabel='Normalized Value')\n",
    "        \n",
    "urlFile = 'BOL.xlsx'\n",
    "sheet_name = 'imp'\n",
    "\n",
    "#ingresar el cluster a analizar (diccionario)\n",
    "cluster = 0 \n",
    "#_namesInClusters = clusters[list(clusters.keys())[cluster-1]]\n",
    "\n",
    "#Arreglo de indicadores si no se utiliza cluster\n",
    "_namesInClusters = [\"NY.TRF.NCTR.KN\",\"NE.CON.GOVT.KD.ZG\"]\n",
    "colEtiquetas = 5\n",
    "normalizacion=True\n",
    "plotDescomposition(urlFile, sheet_name, _namesInClusters, cluster, colEtiquetas, freq=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################################################END#############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reporte numero de indicadores afectados implicitamente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devuelve el numero de indicadores a los que representa un cluster\n",
    "def reporteClusterSectors(urlFile, _namesInClusters, nameMostRepresentative, pais):\n",
    "    xlsx = excelFile(urlFile)\n",
    "    indicator = pd.read_excel(xlsx, sheet_name=\"{0}\".format(_namesInClusters[0])).set_index(\"year\").columns\n",
    "    sector = [_namesInClusters[0]]*len(indicator)\n",
    "    for data in range(1,len(_namesInClusters)):\n",
    "        aux=pd.read_excel(xlsx, sheet_name=\"{0}\".format(_namesInClusters[data])).set_index(\"year\").columns\n",
    "        sector = np.concatenate([sector,[_namesInClusters[data]]*len(aux)], axis=0)\n",
    "        indicator = np.concatenate([indicator,aux],axis=0)  \n",
    "    rep = readFile(nameMostRepresentative, sheet=pais)\n",
    "    ind_sec = [[ind, sec] for ind, sec in zip(indicator, sector) if ind in rep.columns ]\n",
    "    indicator,sector =zip(*ind_sec)\n",
    "    indicadores = len(rep)-rep.loc[:,indicator].isna().sum()\n",
    "    indicadores = pd.DataFrame(indicadores, columns=[\"Nro. subindicadores\"])\n",
    "    indicadores.insert(loc=0, column='Sector', value=list(sector)) \n",
    "    indicadores = indicadores.sort_values(by=\"Nro. subindicadores\", ascending=False)\n",
    "    total = indicadores.iloc[:,1].sum()\n",
    "    indicadores.loc[\"Total\"] = [None,total]\n",
    "    indicadores.index.name=\"Indicadores\"\n",
    "    return  indicadores\n",
    "#urlFile = 'sector_BRANORM.xlsx'\n",
    "#cluster = 2\n",
    "#_namesInClusters = [i for i in clusters[list(clusters.keys())[cluster-1]]]\n",
    "#nameMostRepresentative = \"reporteMostRepresentative_prob0.75.xlsx\"\n",
    "#pais = \"BRA\"\n",
    "#a = reporteClusterSectors(urlFile, _namesInClusters, nameMostRepresentative, pais)\n",
    "#saveFile(a, \"{}_r.xlsx\".format(pais))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#devuelve el numero de indicadores a los que representa un cluster\n",
    "def reporteClusterIndicators(urlFile, sheet_name, _namesInClusters, nameMostRepresentative):\n",
    "    xlsx = excelFile(urlFile)\n",
    "    indicator = pd.read_excel(xlsx, sheet_name=sheet_name).set_index(\"year\").loc[:,_namesInClusters].columns\n",
    "    rep = readFile(nameMostRepresentative, sheet=sheet_name)\n",
    "    indicadores = len(rep)-rep.loc[:,indicator].isna().sum()\n",
    "    indicadores = pd.DataFrame(indicadores, columns=[\"Nro. subindicadores\"])\n",
    "    indicadores = indicadores.sort_values(by=\"Nro. subindicadores\", ascending=False)\n",
    "    total = indicadores.iloc[:,0].sum()\n",
    "    indicadores.loc[\"Total\"] = [total]\n",
    "    indicadores.index.name=\"Indicadores\"\n",
    "    return  indicadores\n",
    "urlFile = 'imputacionCHLNORM.xlsx'\n",
    "cluster = 2\n",
    "_namesInClusters = clusters[list(clusters.keys())[cluster-1]]\n",
    "nameMostRepresentative = \"reporteMostRepresentative_prob0.75.xlsx\"\n",
    "sheet_name = \"CHL\"\n",
    "a = reporteClusterIndicators(urlFile, sheet_name, _namesInClusters, nameMostRepresentative)\n",
    "saveFile(a, \"{}_r.xlsx\".format(sheet_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reporte sector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recuperarIndicadorPorSector1:Recuperamos los indicadores de un sector y como se han distribuido atravez de los otros sectores\n",
    "#PARAMS:\n",
    "#_frameMostRepresentative = dataFrame reporteMostRepresentative_prob0.75.xlsx\n",
    "#_frameSectores = dtaFrame seriesCETS.csv\n",
    "#_framePais = dataFRame imputacionCHLNORM.xlsx\n",
    "#_agrupacion = \"Topic\" dato mediante el cual realiza las agrupaciones en _frameSectores\n",
    "#_sector = \"gender\" sector de la cual se quiere ver como se ha distribuido en los otros indicadores\n",
    "def ubicarSubindicadores(mostRepresentative, indicators):\n",
    "    bools = {}\n",
    "    for ind in mostRepresentative:\n",
    "        aux = list(mostRepresentative[ind][mostRepresentative[ind].isin(indicators)])\n",
    "        if len(aux)>0:\n",
    "            bools[ind]=aux\n",
    "    return bools\n",
    "\n",
    "def buscarIndicadores1(_dataFrameColumns,_indicadores):\n",
    "    indexs = []\n",
    "    for index in _dataFrameColumns:\n",
    "        val = np.where(index==_indicadores)[0]\n",
    "        if val!=None:\n",
    "            indexs.append(list(val)[0])\n",
    "    return _indicadores[indexs]\n",
    "\n",
    "def recuperarIndicadorPorSector1(_frameMostRepresentative, _frameSectores, _framePais, _agrupacion, _sector):\n",
    "    with pd.ExcelWriter('datos_reales/spread_{0}.xlsx'.format(_sector)) as writer:\n",
    "        sectores = _frameSectores.groupby(_agrupacion)\n",
    "        indicadores = None\n",
    "        for index, group in sectores:\n",
    "            if index.lower() == _sector:\n",
    "                indicadores =ubicarSubindicadores(_frameMostRepresentative,np.array(group.iloc[:,0]))\n",
    "        aux_df = pd.DataFrame()\n",
    "        for index, group in sectores:\n",
    "            grupo = buscarIndicadores1(indicadores.keys(), np.array(group.iloc[:,0]))\n",
    "            aux_grupo = {}\n",
    "            maxs = 0\n",
    "            if len(grupo)>0:\n",
    "                for i in grupo:\n",
    "                    if maxs<len(indicadores[i]):\n",
    "                        maxs = len(indicadores[i])\n",
    "                    aux_grupo[i]=indicadores[i]\n",
    "                for key,values in aux_grupo.items():    \n",
    "                    array = values\n",
    "                    end = [None] * (maxs - len(array))\n",
    "                    if len(array) <= maxs:\n",
    "                        array = np.concatenate([array,end])\n",
    "                    aux_grupo[key]=array\n",
    "            \n",
    "                df = pd.DataFrame(aux_grupo)\n",
    "                df.columns = pd.MultiIndex.from_product([[str(index).lower()],df.columns])\n",
    "                \n",
    "                if aux_df.empty:\n",
    "                    aux_df = df\n",
    "                else:\n",
    "                    aux_df = pd.concat([aux_df,df], axis=1)\n",
    "        aux_df.T.to_excel(writer, sheet_name=_sector, index=True)\n",
    "_frameMostRepresentative = readFile('reporteMostRepresentative_prob0.75.xlsx', sheet=\"BRA\")           \n",
    "_frameSectores = readFile('seriesCETS.csv')\n",
    "_framePais = readFile(\"imputacionBRANORM.xlsx\", \"BRA\").set_index(\"year\")\n",
    "_agrupacion = \"Topic\"\n",
    "_sector = \"environment\"\n",
    "recuperarIndicadorPorSector1(_frameMostRepresentative, _frameSectores, _framePais, _agrupacion, _sector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODIGO POR REVISAR\n",
    "series = readFile(\"wdi_0.1.xlsx\",\"Data\").groupby(by=\"Country Code\")\n",
    "indicators = [\"SE.SEC.AGES\", \"SE.PRM.DURS\", \"SE.PRM.ENRR\",\"SE.PRM.ENRR.FE\", \"SE.PRM.ENRR.MA\", \"SE.SEC.DURS\"]\n",
    "series = series.get_group(\"ARG\").reset_index()\n",
    "series = series.iloc[series[\"Indicator Code\"][series[\"Indicator Code\"].isin(indicators)].index, 37:60]\n",
    "font_size = 16\n",
    "hfont = {'fontname':'Arial'}\n",
    "plt.figure(figsize=(15,5))\n",
    "ciclo, tend = sm.tsa.filters.hpfilter(normalizeData(series.T.interpolate()), lamb=6.25)\n",
    "plt.subplot()\n",
    "desc = None\n",
    "opcion = 0\n",
    "if opcion ==0:\n",
    "    desc = tend\n",
    "    plt.title((\"Trend\").title(), fontsize=font_size, **hfont)\n",
    "if opcion ==1:\n",
    "    desc = ciclo\n",
    "    plt.title((\"Cycle\").title(), fontsize=font_size, **hfont)\n",
    "\n",
    "        \n",
    "plt.xlabel(\"Year\", fontsize=font_size, **hfont)\n",
    "plt.ylabel(\"Normalized Value\", fontsize=font_size, **hfont)\n",
    "vmin = math.floor(desc.min().min())\n",
    "vmax = math.ceil(desc.max().max())\n",
    "plt.ylim(vmin, vmax)\n",
    "for i in desc:\n",
    "    plt.plot(desc.loc[:,i],label=str(i), marker='', linewidth=1.5)\n",
    "col = math.ceil(len(desc)/colEtiquetas)\n",
    "plt.legend(bbox_to_anchor=(0.0, 0.29, 1, 1), loc='upper center', ncol=math.ceil(len(desc)/col), mode=\"expand\", borderaxespad=0)\n",
    "plt.grid(True)   \n",
    "plt.show()\n",
    "#\n",
    "#series = series.iloc[series[\"Indicator Code\"][series[\"Indicator Code\"].isin(indicators)].index, 36:60]\n",
    "#series"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
