{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os,inspect\n",
    "import h5py\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "#Capas\n",
    "from keras.layers import Input\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense, Flatten\n",
    "from keras.layers import Dropout\n",
    "#Optimizadores\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "#utilidades\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from tqdm import tqdm\n",
    "#utilitis\n",
    "from keras.utils import plot_model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "from preprocesamiento import data\n",
    "from keras.models import load_model, Model\n",
    "import keras.backend as K\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#datos historicos\n",
    "n_in=4\n",
    "#datos predecidos\n",
    "n_out=1\n",
    "#porcentage de datos para el entrenamiento\n",
    "n_train=85\n",
    "(x_train, y_train, x_val, y_val),var = data('union.xlsx', n_in, n_out, n_train,  \"a\")\n",
    "n_in=4*var\n",
    "n_out=n_out*var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train=x_train.reshape((-1,x_train.shape[2], x_train.shape[1]))\n",
    "x_val=x_val.reshape((-1,x_val.shape[2], x_val.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(step_input=1,\n",
    "              step_out=n_out,\n",
    "              size_input=n_in,\n",
    "              output_dim=1,\n",
    "              encoder_units=64):\n",
    "    '''\n",
    "            step_input: longitud de la entrada hacia el lstm(datos pasados para predecir un valor futuro)\n",
    "              step_out: alcanze de la inferencia, en nuestro caso se infiere un año,\n",
    "              size_input: tamaño de la entrada de los datos,\n",
    "              output_dim: tamaño de la salida de los datos,\n",
    "              encoder_units: espacio dimencional de salida de la LSTM\n",
    "    '''\n",
    "    X = Input(shape = (step_input, size_input))\n",
    "    #primera capa de LSTM\n",
    "    #lstm= LSTM(encoder_units, activation=\"tanh\", return_sequences=False)(X)\n",
    "    #lstm= LSTM(encoder_units, activation=\"tanh\", return_sequences=False)(lstm)\n",
    "    #capa fully conected con una neurona\n",
    "    out = Dense(64, activation='tanh')(X)\n",
    "    out = Flatten()(out)\n",
    "    out = Dense(12, activation='tanh')(out)\n",
    "    model = Model(inputs = [X], outputs = out)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot model seq to seq (LSTM) with attention and save img in root directory\n",
    "plot_model(model, to_file='modeloRNN.png', show_shapes=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model \n",
    "#se utiliza el optimizador root mean square con los siguientes hyperparametros\n",
    "#Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#RMSprop(lr=0.45, rho=0.94, epsilon=1.0, decay=0.1)\n",
    "opt = Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "#se utiliza la tecnica de early stoping con el objetivo de evitar el sobre ajuste de la red, y deteniendo\n",
    "#el proceso se detiene despues de 4 itereaciones si no existiera una mejora\n",
    "early_stopping=EarlyStopping(monitor='val_loss', patience=4)\n",
    "#se guarda los pesos del modelo como pesos.h5 si en cada paso de todo el conjunto de datos existiera una mejora\n",
    "mcp_save = ModelCheckpoint('pesos.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min')\n",
    "#las metricas de entrenamiento y la perdida se configuran con mean square error\n",
    "model.compile(optimizer=opt, metrics=[\"mae\"], loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "History=model.fit(x_train, y_train, epochs=1000,  callbacks=[early_stopping,mcp_save], batch_size=4, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graficas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprime las variables de medicion que serviran para graficar \n",
    "print(History.history.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot train and validation loss\n",
    "plt.plot(History.history['loss'])\n",
    "plt.plot(History.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['loss', 'val_loss'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Se realiza la inferencia del modelo con los datos de testing\n",
    "# en nuestro caso se uso con los datos de validacion por no contar con suficiente cantidad de datos(este proceso es algo forzado y erroneo)\n",
    "inf = model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imprime los valores inferidos y los datos de testing original(en nuestro caso se uso los datos de validacion)\n",
    "inf=inf.T\n",
    "y_val =y_val.T\n",
    "print(\"{0}-{1}\".format(inf,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grafica los datos inferidos conjuntamente con los datos de testing(en nuestrp caso los datos de validacion)\n",
    "n = 6\n",
    "plt.plot(inf[n])\n",
    "plt.plot(y_val[n])\n",
    "plt.title('Inference')\n",
    "plt.ylabel('Value')\n",
    "plt.xlabel('Data')\n",
    "plt.legend(['prediction', 'real_data'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean GPU memory\n",
    "def limit_mem():\n",
    "    K.get_session().close()\n",
    "    cfg = tf.ConfigProto()\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    tf.Session(config=cfg)\n",
    "limit_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer learning\n",
    "Existe diferentes maneras de aplicar transferencia de aprendisaje a un modelo entrenado, en nuestro caso se entreno el modelo completo puesto que solo esta formado por una capa de LSTM, y no se aplican varios conceptos pertienentes a la transferencia de aprendisaje por razones de la limitada cantidad de informacion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#carga los pesos entrenados del modelo para su posterior uso de prueba\n",
    "#observacion: si se se desea realizar transferencia de aprendisaje es necesario entrenar el modelo con un dataset diferente al original\n",
    "path_file = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\n",
    "WEIGHTS_PATH = os.path.join(path_file,'pesos/pesos.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(step_input=n_in,\n",
    "              step_out=n_out,\n",
    "              size_input=1,\n",
    "              output_dim=1,\n",
    "              encoder_units=50):\n",
    "    '''\n",
    "            step_input: longitud de la entrada hacia el lstm(datos pasados para predecir un valor futuro)\n",
    "              step_out: alcanze de la inferencia, en nuestro caso se infiere un año,\n",
    "              size_input: tamaño de la entrada de los datos,\n",
    "              output_dim: tamaño de la salida de los datos,\n",
    "              encoder_units: espacio dimencional de salida de la LSTM\n",
    "    '''\n",
    "    X = Input(shape = (step_input, size_input))\n",
    "    #primera capa de LSTM\n",
    "    lstm= LSTM(encoder_units, activation=\"tanh\", return_sequences=False)(X)\n",
    "    #capa fully conected con una neurona\n",
    "    out = Dense(1)(lstm)\n",
    "    model = Model(inputs = [X], outputs = out)\n",
    "    ### END CODE HERE ###\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(WEIGHTS_PATH, by_name = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile model(estos datos pueden cambiar no siempre es identico al entrenamiento, se recomienda disminuir la tasa de aprendisaje)\n",
    "#se utiliza el optimizador root mean square con los siguientes hyperparametros\n",
    "opt = RMSprop(lr=0.45, rho=0.94, epsilon=1.0, decay=0.1)\n",
    "#se utiliza la tecnica de early stoping con el objetivo de evitar el sobre ajuste de la red, y deteniendo\n",
    "#el proceso se detiene despues de 4 itereaciones si no existiera una mejora\n",
    "early_stopping=EarlyStopping(monitor='val_loss', patience=4)\n",
    "#se guarda los pesos del modelo como pesos.h5 si en cada paso de todo el conjunto de datos existiera una mejora\n",
    "mcp_save = ModelCheckpoint('pesos.h5', save_best_only=True, save_weights_only=True, monitor='val_loss', mode='min')\n",
    "#las metricas de entrenamiento y la perdida se configuran con mean square error\n",
    "model.compile(optimizer=opt, metrics=[\"mae\"], loss='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train model\n",
    "History=model.fit(x_train, y_train, epochs=1000,  callbacks=[early_stopping,mcp_save], batch_size=4, validation_data=(x_val,y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modulo de limpieza de memoria del GPU\n",
    "def limit_mem():\n",
    "    K.get_session().close()\n",
    "    cfg = K.tf.ConfigProto()\n",
    "    cfg.gpu_options.allow_growth = True\n",
    "    K.set_session(K.tf.Session(config=cfg))\n",
    "limit_mem()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
